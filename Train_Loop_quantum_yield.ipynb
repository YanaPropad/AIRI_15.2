{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from schnetpack.nn import build_mlp\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "from typing import Sequence, Union, Callable, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import schnetpack as spk\n",
    "import schnetpack.nn as snn\n",
    "import schnetpack.properties as properties\n",
    "\n",
    "# from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maligina/yana/gnn\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conformers_1_chromophore_train.db  train_clean.csv\n",
      "test_clean.csv\t\t\t   train_clean_scalar_painn_test.pkl\n",
      "test_clean_scalar_painn.pkl\t   train_clean_scalar_painn_train.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target: Absorption_max_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('/home/propad/gnn/DimeNet/embeddings_DimeNet/train_clean_scalar_dimenet_train.pkl')\n",
    "test_data = pd.read_pickle('/home/propad/gnn/DimeNet/embeddings_DimeNet/train_clean_scalar_dimenet_test.pkl')\n",
    "test_eval = pd.read_pickle('/home/propad/gnn/DimeNet/embeddings_DimeNet/test_clean_scalar_dimenet.pkl')\n",
    "\n",
    "train_target_log = torch.FloatTensor(np.log(train_data['Quantum_yield'].to_numpy() + 1e-8))\n",
    "test_target_log = torch.FloatTensor(np.log(test_data['Quantum_yield'].to_numpy() + 1e-8))\n",
    "test_eval_target_log = torch.FloatTensor(np.log(test_eval['Quantum_yield'].to_numpy() + 1e-8))\n",
    "\n",
    "train_tensor = torch.stack(train_data['Concatenated_embedding'].to_list(), dim=0)\n",
    "# train_target = torch.FloatTensor(train_data['Quantum_yield'].to_list())\n",
    "# train_target_norm = (train_target - train_target.mean()) / train_target.std()\n",
    "train_dataset = TensorDataset(train_tensor, train_target_log)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_tensor = torch.stack(test_data['Concatenated_embedding'].to_list(), dim=0)\n",
    "# test_target = torch.FloatTensor(test_data['Quantum_yield'].to_list())\n",
    "# test_target_norm = (test_target - test_target.mean()) / test_target.std()\n",
    "test_dataset = TensorDataset(test_tensor, test_target_log)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_eval_tensor = torch.stack(test_eval['Concatenated_embedding'].to_list(), dim=0)\n",
    "# test_eval_target = torch.FloatTensor(test_eval['Quantum_yield'].to_list())\n",
    "# test_eval_target_norm = (test_eval_target - test_eval_target.mean()) / test_eval_target.std()\n",
    "test_eval_dataset = TensorDataset(test_eval_tensor, test_eval_target_log)\n",
    "test_eval_loader = DataLoader(test_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in inputs at batch 4\n",
      "NaN detected in inputs at batch 5\n",
      "NaN detected in inputs at batch 6\n",
      "NaN detected in inputs at batch 8\n",
      "NaN detected in inputs at batch 14\n",
      "NaN detected in inputs at batch 16\n",
      "NaN detected in inputs at batch 19\n",
      "NaN detected in inputs at batch 20\n",
      "NaN detected in inputs at batch 21\n",
      "NaN detected in inputs at batch 22\n",
      "NaN detected in inputs at batch 23\n",
      "NaN detected in inputs at batch 26\n",
      "NaN detected in inputs at batch 28\n",
      "NaN detected in inputs at batch 30\n",
      "NaN detected in inputs at batch 31\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    inputs, labels = batch\n",
    "    \n",
    "    # Check for NaN values in inputs and labels\n",
    "    if torch.isnan(inputs).any():\n",
    "        print(f\"NaN detected in inputs at batch {i}\")\n",
    "        \n",
    "    if torch.isnan(labels).any():\n",
    "        print(f\"NaN detected in labels at batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_hidden, n_layers, activation):\n",
    "        super().__init__()\n",
    "        self.mlp = spk.nn.build_mlp(\n",
    "            n_in=n_in,\n",
    "            n_out=n_out,\n",
    "            n_hidden=n_hidden,\n",
    "            n_layers=n_layers,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        return self.mlp(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPModel(\n",
       "  (mlp): Sequential(\n",
       "    (0): Dense(in_features=256, out_features=512, bias=True)\n",
       "    (1): Dense(in_features=512, out_features=256, bias=True)\n",
       "    (2): Dense(in_features=256, out_features=128, bias=True)\n",
       "    (3): Dense(\n",
       "      in_features=128, out_features=1, bias=True\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPModel(256, 1, [512, 256, 128], n_layers=4, activation=F.silu)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs.squeeze(1), labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 5.030104764699936\n",
      "  batch 200 loss: 3.101677006483078\n",
      "LOSS train 3.101677006483078 valid 3.4968782290816307\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 2.9548147308826445\n",
      "  batch 200 loss: 2.9960317468643187\n",
      "LOSS train 2.9960317468643187 valid 3.5835913717746735\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 2.9236416530609133\n",
      "  batch 200 loss: 2.850365616083145\n",
      "LOSS train 2.850365616083145 valid 3.3583860248327255\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 2.7675919771194457\n",
      "  batch 200 loss: 2.7442680168151856\n",
      "LOSS train 2.7442680168151856 valid 3.1529356874525547\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 2.652113072872162\n",
      "  batch 200 loss: 2.715234125852585\n",
      "LOSS train 2.715234125852585 valid 3.019937474280596\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 2.525844393968582\n",
      "  batch 200 loss: 2.680077325105667\n",
      "LOSS train 2.680077325105667 valid 2.872957222163677\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 2.5886574637889863\n",
      "  batch 200 loss: 2.4506946337223052\n",
      "LOSS train 2.4506946337223052 valid 2.962350271642208\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 2.516972620487213\n",
      "  batch 200 loss: 2.559340924024582\n",
      "LOSS train 2.559340924024582 valid 2.9496668353676796\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 2.3783310502767563\n",
      "  batch 200 loss: 2.360154978632927\n",
      "LOSS train 2.360154978632927 valid 3.3169731870293617\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 2.3811182272434235\n",
      "  batch 200 loss: 2.612409115433693\n",
      "LOSS train 2.612409115433693 valid 2.741583187133074\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 2.3427930128574372\n",
      "  batch 200 loss: 2.4254963392019273\n",
      "LOSS train 2.4254963392019273 valid 2.670435916632414\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 2.3620145356655122\n",
      "  batch 200 loss: 2.424320548772812\n",
      "LOSS train 2.424320548772812 valid 2.7306006252765656\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 2.1835371160507204\n",
      "  batch 200 loss: 2.4453330516815184\n",
      "LOSS train 2.4453330516815184 valid 2.8629106022417545\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 2.293118662238121\n",
      "  batch 200 loss: 2.2944470942020416\n",
      "LOSS train 2.2944470942020416 valid 2.663608107715845\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 2.240196968317032\n",
      "  batch 200 loss: 2.1343449431657793\n",
      "LOSS train 2.1343449431657793 valid 2.640048399567604\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 2.0906635195016863\n",
      "  batch 200 loss: 2.190704538822174\n",
      "LOSS train 2.190704538822174 valid 2.6782773546874523\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 2.1615737426280974\n",
      "  batch 200 loss: 2.117544792890549\n",
      "LOSS train 2.117544792890549 valid 2.6969738639891148\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 2.125984528064728\n",
      "  batch 200 loss: 2.09775025844574\n",
      "LOSS train 2.09775025844574 valid 2.5194854363799095\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 2.18648430287838\n",
      "  batch 200 loss: 2.0293385690450667\n",
      "LOSS train 2.0293385690450667 valid 2.4108085595071316\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 2.0455997294187545\n",
      "  batch 200 loss: 2.0834247708320617\n",
      "LOSS train 2.0834247708320617 valid 2.5067146494984627\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 2.057466880083084\n",
      "  batch 200 loss: 1.9151397302746773\n",
      "LOSS train 1.9151397302746773 valid 2.287834893912077\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 1.9565848034620286\n",
      "  batch 200 loss: 2.100817776918411\n",
      "LOSS train 2.100817776918411 valid 2.347887746989727\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 1.9816317135095596\n",
      "  batch 200 loss: 2.006165207028389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 2.006165207028389 valid 3.2497046440839767\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 1.9236564546823502\n",
      "  batch 200 loss: 1.9558195996284484\n",
      "LOSS train 1.9558195996284484 valid 2.306155990809202\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 1.7678364706039429\n",
      "  batch 200 loss: 1.9628013616800308\n",
      "LOSS train 1.9628013616800308 valid 3.34450376778841\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 1.9352432298660278\n",
      "  batch 200 loss: 1.8931133562326432\n",
      "LOSS train 1.8931133562326432 valid 2.390801567584276\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 1.7520808047056198\n",
      "  batch 200 loss: 1.8424766558408736\n",
      "LOSS train 1.8424766558408736 valid 2.235000103712082\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 1.79238542675972\n",
      "  batch 200 loss: 1.7442343705892562\n",
      "LOSS train 1.7442343705892562 valid 2.255708072334528\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 1.7321607428789139\n",
      "  batch 200 loss: 1.7144927549362183\n",
      "LOSS train 1.7144927549362183 valid 2.229382835328579\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 1.8508217525482178\n",
      "  batch 200 loss: 1.6834259235858917\n",
      "LOSS train 1.6834259235858917 valid 2.167241184040904\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 1.637600473165512\n",
      "  batch 200 loss: 1.6783849865198135\n",
      "LOSS train 1.6783849865198135 valid 2.333475712686777\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 1.6162802791595459\n",
      "  batch 200 loss: 1.7689344990253448\n",
      "LOSS train 1.7689344990253448 valid 2.280607810243964\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 1.6252354776859284\n",
      "  batch 200 loss: 1.57892931163311\n",
      "LOSS train 1.57892931163311 valid 2.114481270313263\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 1.6191769689321518\n",
      "  batch 200 loss: 1.5779048717021942\n",
      "LOSS train 1.5779048717021942 valid 2.146452594548464\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 1.6035395431518555\n",
      "  batch 200 loss: 1.517114120721817\n",
      "LOSS train 1.517114120721817 valid 2.1466790437698364\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 1.5728947573900223\n",
      "  batch 200 loss: 1.560513660311699\n",
      "LOSS train 1.560513660311699 valid 2.269777748733759\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 1.4301606491208076\n",
      "  batch 200 loss: 1.5968367874622345\n",
      "LOSS train 1.5968367874622345 valid 2.0929162334650755\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 1.4842983961105347\n",
      "  batch 200 loss: 1.5090804731845855\n",
      "LOSS train 1.5090804731845855 valid 2.0681548938155174\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 1.5014348137378692\n",
      "  batch 200 loss: 1.4833510994911194\n",
      "LOSS train 1.4833510994911194 valid 2.103896349668503\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 1.5105127239227294\n",
      "  batch 200 loss: 1.4306900951266288\n",
      "LOSS train 1.4306900951266288 valid 2.4010440092533827\n",
      "EPOCH 41:\n",
      "  batch 100 loss: 1.4843729740381242\n",
      "  batch 200 loss: 1.467758641242981\n",
      "LOSS train 1.467758641242981 valid 1.977393314242363\n",
      "EPOCH 42:\n",
      "  batch 100 loss: 1.3606933563947679\n",
      "  batch 200 loss: 1.4089437413215637\n",
      "LOSS train 1.4089437413215637 valid 2.0303362235426903\n",
      "EPOCH 43:\n",
      "  batch 100 loss: 1.43498138487339\n",
      "  batch 200 loss: 1.319662954211235\n",
      "LOSS train 1.319662954211235 valid 2.084886321797967\n",
      "EPOCH 44:\n",
      "  batch 100 loss: 1.29589713037014\n",
      "  batch 200 loss: 1.3927926075458528\n",
      "LOSS train 1.3927926075458528 valid 1.9692727141082287\n",
      "EPOCH 45:\n",
      "  batch 100 loss: 1.4009331530332565\n",
      "  batch 200 loss: 1.363198910355568\n",
      "LOSS train 1.363198910355568 valid 1.9877661317586899\n",
      "EPOCH 46:\n",
      "  batch 100 loss: 1.3297973304986954\n",
      "  batch 200 loss: 1.499376096725464\n",
      "LOSS train 1.499376096725464 valid 2.23599536716938\n",
      "EPOCH 47:\n",
      "  batch 100 loss: 1.3169705653190613\n",
      "  batch 200 loss: 1.3362214305996896\n",
      "LOSS train 1.3362214305996896 valid 2.0115354768931866\n",
      "EPOCH 48:\n",
      "  batch 100 loss: 1.3832919147610665\n",
      "  batch 200 loss: 1.3112390238046645\n",
      "LOSS train 1.3112390238046645 valid 2.056807393208146\n",
      "EPOCH 49:\n",
      "  batch 100 loss: 1.2559634524583816\n",
      "  batch 200 loss: 1.449982481598854\n",
      "LOSS train 1.449982481598854 valid 1.986372385174036\n",
      "EPOCH 50:\n",
      "  batch 100 loss: 1.1635062316060065\n",
      "  batch 200 loss: 1.2941550475358963\n",
      "LOSS train 1.2941550475358963 valid 2.0548987854272127\n",
      "EPOCH 51:\n",
      "  batch 100 loss: 1.193303128182888\n",
      "  batch 200 loss: 1.3209167557954788\n",
      "LOSS train 1.3209167557954788 valid 2.05513371899724\n",
      "EPOCH 52:\n",
      "  batch 100 loss: 1.2105480232834815\n",
      "  batch 200 loss: 1.227955983877182\n",
      "LOSS train 1.227955983877182 valid 2.0507923625409603\n",
      "EPOCH 53:\n",
      "  batch 100 loss: 1.2595132166147232\n",
      "  batch 200 loss: 1.1378570529818535\n",
      "LOSS train 1.1378570529818535 valid 2.0219504684209824\n",
      "EPOCH 54:\n",
      "  batch 100 loss: 1.2018779620528222\n",
      "  batch 200 loss: 1.1748007369041442\n",
      "LOSS train 1.1748007369041442 valid 2.0167599469423294\n",
      "EPOCH 55:\n",
      "  batch 100 loss: 1.1464723929762841\n",
      "  batch 200 loss: 1.1684893265366554\n",
      "LOSS train 1.1684893265366554 valid 1.9252909887582064\n",
      "EPOCH 56:\n",
      "  batch 100 loss: 1.1356770956516267\n",
      "  batch 200 loss: 1.154235818386078\n",
      "LOSS train 1.154235818386078 valid 2.060555076226592\n",
      "EPOCH 57:\n",
      "  batch 100 loss: 1.1599748116731643\n",
      "  batch 200 loss: 1.153265399336815\n",
      "LOSS train 1.153265399336815 valid 2.0001084730029106\n",
      "EPOCH 58:\n",
      "  batch 100 loss: 1.1552379143238067\n",
      "  batch 200 loss: 1.0578258258104325\n",
      "LOSS train 1.0578258258104325 valid 1.8546518869698048\n",
      "EPOCH 59:\n",
      "  batch 100 loss: 1.0609261727333068\n",
      "  batch 200 loss: 1.1497325319051743\n",
      "LOSS train 1.1497325319051743 valid 1.9548824541270733\n",
      "EPOCH 60:\n",
      "  batch 100 loss: 1.1042303955554962\n",
      "  batch 200 loss: 1.0891599428653718\n",
      "LOSS train 1.0891599428653718 valid 1.8732610680162907\n",
      "EPOCH 61:\n",
      "  batch 100 loss: 1.0886151665449142\n",
      "  batch 200 loss: 1.0681339654326438\n",
      "LOSS train 1.0681339654326438 valid 1.9081636052578688\n",
      "EPOCH 62:\n",
      "  batch 100 loss: 1.145404545068741\n",
      "  batch 200 loss: 1.0025820037722588\n",
      "LOSS train 1.0025820037722588 valid 1.8721417859196663\n",
      "EPOCH 63:\n",
      "  batch 100 loss: 1.01455707103014\n",
      "  batch 200 loss: 1.0751110765337943\n",
      "LOSS train 1.0751110765337943 valid 1.8029596265405416\n",
      "EPOCH 64:\n",
      "  batch 100 loss: 1.025076539814472\n",
      "  batch 200 loss: 1.0568387520313263\n",
      "LOSS train 1.0568387520313263 valid 1.8009014781564474\n",
      "EPOCH 65:\n",
      "  batch 100 loss: 0.9084556782245636\n",
      "  batch 200 loss: 1.034269796013832\n",
      "LOSS train 1.034269796013832 valid 1.9035280533134937\n",
      "EPOCH 66:\n",
      "  batch 100 loss: 1.0043340089917183\n",
      "  batch 200 loss: 0.9935757026076317\n",
      "LOSS train 0.9935757026076317 valid 1.8891055081039667\n",
      "EPOCH 67:\n",
      "  batch 100 loss: 0.9973424568772316\n",
      "  batch 200 loss: 0.9593855878710746\n",
      "LOSS train 0.9593855878710746 valid 1.9467530213296413\n",
      "EPOCH 68:\n",
      "  batch 100 loss: 0.9430139964818954\n",
      "  batch 200 loss: 1.0395975551009178\n",
      "LOSS train 1.0395975551009178 valid 1.8046269491314888\n",
      "EPOCH 69:\n",
      "  batch 100 loss: 0.9977827870845795\n",
      "  batch 200 loss: 1.0192973047494889\n",
      "LOSS train 1.0192973047494889 valid 1.7633576169610023\n",
      "EPOCH 70:\n",
      "  batch 100 loss: 0.930948615372181\n",
      "  batch 200 loss: 0.9702342775464058\n",
      "LOSS train 0.9702342775464058 valid 1.8003346659243107\n",
      "EPOCH 71:\n",
      "  batch 100 loss: 0.8801250270009041\n",
      "  batch 200 loss: 0.8918214794993401\n",
      "LOSS train 0.8918214794993401 valid 1.890379911288619\n",
      "EPOCH 72:\n",
      "  batch 100 loss: 0.911398454606533\n",
      "  batch 200 loss: 0.9073180392384529\n",
      "LOSS train 0.9073180392384529 valid 1.8406745940446854\n",
      "EPOCH 73:\n",
      "  batch 100 loss: 0.8815968286991119\n",
      "  batch 200 loss: 0.8891071835160256\n",
      "LOSS train 0.8891071835160256 valid 1.7422773633152246\n",
      "EPOCH 74:\n",
      "  batch 100 loss: 0.8336006808280945\n",
      "  batch 200 loss: 0.8678721508383751\n",
      "LOSS train 0.8678721508383751 valid 1.8235260657966137\n",
      "EPOCH 75:\n",
      "  batch 100 loss: 0.8669014114141464\n",
      "  batch 200 loss: 0.9868562817573547\n",
      "LOSS train 0.9868562817573547 valid 1.7229785863310099\n",
      "EPOCH 76:\n",
      "  batch 100 loss: 0.7967236137390137\n",
      "  batch 200 loss: 0.9257854548096657\n",
      "LOSS train 0.9257854548096657 valid 1.7656887602061033\n",
      "EPOCH 77:\n",
      "  batch 100 loss: 0.8091901978850364\n",
      "  batch 200 loss: 0.8315186774730683\n",
      "LOSS train 0.8315186774730683 valid 1.7626777868717909\n",
      "EPOCH 78:\n",
      "  batch 100 loss: 0.8273878714442253\n",
      "  batch 200 loss: 0.8316299822926522\n",
      "LOSS train 0.8316299822926522 valid 1.6960562020540237\n",
      "EPOCH 79:\n",
      "  batch 100 loss: 0.7937668418884277\n",
      "  batch 200 loss: 0.8919343227148055\n",
      "LOSS train 0.8919343227148055 valid 1.748826913535595\n",
      "EPOCH 80:\n",
      "  batch 100 loss: 0.8411226639151573\n",
      "  batch 200 loss: 0.8359135863184929\n",
      "LOSS train 0.8359135863184929 valid 1.7124829981476068\n",
      "EPOCH 81:\n",
      "  batch 100 loss: 0.7781938713788986\n",
      "  batch 200 loss: 0.9331826266646385\n",
      "LOSS train 0.9331826266646385 valid 1.6569557823240757\n",
      "EPOCH 82:\n",
      "  batch 100 loss: 0.7594516003131866\n",
      "  batch 200 loss: 0.7614565497636795\n",
      "LOSS train 0.7614565497636795 valid 1.7659585233777761\n",
      "EPOCH 83:\n",
      "  batch 100 loss: 0.7722644090652466\n",
      "  batch 200 loss: 0.7961920392513275\n",
      "LOSS train 0.7961920392513275 valid 1.7361309919506311\n",
      "EPOCH 84:\n",
      "  batch 100 loss: 0.7242299509048462\n",
      "  batch 200 loss: 0.7922270855307579\n",
      "LOSS train 0.7922270855307579 valid 1.671653663739562\n",
      "EPOCH 85:\n",
      "  batch 100 loss: 0.7739652359485626\n",
      "  batch 200 loss: 0.8606949673593044\n",
      "LOSS train 0.8606949673593044 valid 1.7811160143464804\n",
      "EPOCH 86:\n",
      "  batch 100 loss: 0.7853895780444146\n",
      "  batch 200 loss: 0.7570236295461654\n",
      "LOSS train 0.7570236295461654 valid 1.616957526654005\n",
      "EPOCH 87:\n",
      "  batch 100 loss: 0.709625506401062\n",
      "  batch 200 loss: 0.7940649980306625\n",
      "LOSS train 0.7940649980306625 valid 1.7271004170179367\n",
      "EPOCH 88:\n",
      "  batch 100 loss: 0.6812923952937127\n",
      "  batch 200 loss: 0.7724377948045731\n",
      "LOSS train 0.7724377948045731 valid 1.6353536173701286\n",
      "EPOCH 89:\n",
      "  batch 100 loss: 0.6986931842565537\n",
      "  batch 200 loss: 0.7201044085621834\n",
      "LOSS train 0.7201044085621834 valid 1.7494234684854746\n",
      "EPOCH 90:\n",
      "  batch 100 loss: 0.6950842723250389\n",
      "  batch 200 loss: 0.7475192895531655\n",
      "LOSS train 0.7475192895531655 valid 1.6826103404164314\n",
      "EPOCH 91:\n",
      "  batch 100 loss: 0.6718356341123581\n",
      "  batch 200 loss: 0.7513401988148689\n",
      "LOSS train 0.7513401988148689 valid 1.6966238617897034\n",
      "EPOCH 92:\n",
      "  batch 100 loss: 0.6799053664505482\n",
      "  batch 200 loss: 0.7337816788256168\n",
      "LOSS train 0.7337816788256168 valid 1.684165507555008\n",
      "EPOCH 93:\n",
      "  batch 100 loss: 0.727156728208065\n",
      "  batch 200 loss: 0.6597197113931179\n",
      "LOSS train 0.6597197113931179 valid 1.650246251374483\n",
      "EPOCH 94:\n",
      "  batch 100 loss: 0.6778599411249161\n",
      "  batch 200 loss: 0.685972100943327\n",
      "LOSS train 0.685972100943327 valid 1.7093687281012535\n",
      "EPOCH 95:\n",
      "  batch 100 loss: 0.674458640217781\n",
      "  batch 200 loss: 0.6984149351716041\n",
      "LOSS train 0.6984149351716041 valid 1.6411267649382353\n",
      "EPOCH 96:\n",
      "  batch 100 loss: 0.6146741962432861\n",
      "  batch 200 loss: 0.6756881925463677\n",
      "LOSS train 0.6756881925463677 valid 1.5812919735908508\n",
      "EPOCH 97:\n",
      "  batch 100 loss: 0.6420004394650459\n",
      "  batch 200 loss: 0.6570957529544831\n",
      "LOSS train 0.6570957529544831 valid 1.6547873970121145\n",
      "EPOCH 98:\n",
      "  batch 100 loss: 0.6690266153216362\n",
      "  batch 200 loss: 0.6605251860618592\n",
      "LOSS train 0.6605251860618592 valid 1.5614619757980108\n",
      "EPOCH 99:\n",
      "  batch 100 loss: 0.5826078256964684\n",
      "  batch 200 loss: 0.6808279457688332\n",
      "LOSS train 0.6808279457688332 valid 1.6505805607885122\n",
      "EPOCH 100:\n",
      "  batch 100 loss: 0.6286357101798058\n",
      "  batch 200 loss: 0.6625406341254712\n",
      "LOSS train 0.6625406341254712 valid 1.6056182533502579\n",
      "EPOCH 101:\n",
      "  batch 100 loss: 0.5998675230145455\n",
      "  batch 200 loss: 0.6687641523778438\n",
      "LOSS train 0.6687641523778438 valid 1.613863131031394\n",
      "EPOCH 102:\n",
      "  batch 100 loss: 0.5848285084962845\n",
      "  batch 200 loss: 0.6515797367691993\n",
      "LOSS train 0.6515797367691993 valid 1.6685745753347874\n",
      "EPOCH 103:\n",
      "  batch 100 loss: 0.5818300198018551\n",
      "  batch 200 loss: 0.6006363493204117\n",
      "LOSS train 0.6006363493204117 valid 1.6145804803818464\n",
      "EPOCH 104:\n",
      "  batch 100 loss: 0.5893740491569042\n",
      "  batch 200 loss: 0.6789215648174286\n",
      "LOSS train 0.6789215648174286 valid 1.6394551414996386\n",
      "EPOCH 105:\n",
      "  batch 100 loss: 0.618399614393711\n",
      "  batch 200 loss: 0.5657559692859649\n",
      "LOSS train 0.5657559692859649 valid 1.5760410465300083\n",
      "EPOCH 106:\n",
      "  batch 100 loss: 0.5954494768381119\n",
      "  batch 200 loss: 0.5912317457795143\n",
      "LOSS train 0.5912317457795143 valid 1.5839163772761822\n",
      "EPOCH 107:\n",
      "  batch 100 loss: 0.5872813941538334\n",
      "  batch 200 loss: 0.5817710168659687\n",
      "LOSS train 0.5817710168659687 valid 1.5853465963155031\n",
      "EPOCH 108:\n",
      "  batch 100 loss: 0.5554369983077049\n",
      "  batch 200 loss: 0.5592639890313148\n",
      "LOSS train 0.5592639890313148 valid 1.6194404624402523\n",
      "EPOCH 109:\n",
      "  batch 100 loss: 0.5569617669284344\n",
      "  batch 200 loss: 0.5810018813610077\n",
      "LOSS train 0.5810018813610077 valid 1.6236717253923416\n",
      "EPOCH 110:\n",
      "  batch 100 loss: 0.445316628664732\n",
      "  batch 200 loss: 0.4849867953360081\n",
      "LOSS train 0.4849867953360081 valid 1.5487453415989876\n",
      "EPOCH 111:\n",
      "  batch 100 loss: 0.46621464028954507\n",
      "  batch 200 loss: 0.4335813769698143\n",
      "LOSS train 0.4335813769698143 valid 1.5091344509273767\n",
      "EPOCH 112:\n",
      "  batch 100 loss: 0.45932826861739157\n",
      "  batch 200 loss: 0.44377035528421405\n",
      "LOSS train 0.44377035528421405 valid 1.4929183553904295\n",
      "EPOCH 113:\n",
      "  batch 100 loss: 0.4341273821890354\n",
      "  batch 200 loss: 0.4457522863149643\n",
      "LOSS train 0.4457522863149643 valid 1.530359461903572\n",
      "EPOCH 114:\n",
      "  batch 100 loss: 0.42857277110219\n",
      "  batch 200 loss: 0.4550841295719147\n",
      "LOSS train 0.4550841295719147 valid 1.516728714108467\n",
      "EPOCH 115:\n",
      "  batch 100 loss: 0.4179895946383476\n",
      "  batch 200 loss: 0.46931621491909026\n",
      "LOSS train 0.46931621491909026 valid 1.556670319288969\n",
      "EPOCH 116:\n",
      "  batch 100 loss: 0.42793184489011765\n",
      "  batch 200 loss: 0.44986760437488554\n",
      "LOSS train 0.44986760437488554 valid 1.5145968105643988\n",
      "EPOCH 117:\n",
      "  batch 100 loss: 0.42554140970110893\n",
      "  batch 200 loss: 0.4535610544681549\n",
      "LOSS train 0.4535610544681549 valid 1.5511600263416767\n",
      "EPOCH 118:\n",
      "  batch 100 loss: 0.41986694172024724\n",
      "  batch 200 loss: 0.4324016797542572\n",
      "LOSS train 0.4324016797542572 valid 1.538580572232604\n",
      "EPOCH 119:\n",
      "  batch 100 loss: 0.403377553075552\n",
      "  batch 200 loss: 0.4438164274394512\n",
      "LOSS train 0.4438164274394512 valid 1.5494176084175706\n",
      "EPOCH 120:\n",
      "  batch 100 loss: 0.39241338297724726\n",
      "  batch 200 loss: 0.44705143362283706\n",
      "LOSS train 0.44705143362283706 valid 1.5051315361633897\n",
      "EPOCH 121:\n",
      "  batch 100 loss: 0.437329453676939\n",
      "  batch 200 loss: 0.4220586670935154\n",
      "LOSS train 0.4220586670935154 valid 1.5091798212379217\n",
      "EPOCH 122:\n",
      "  batch 100 loss: 0.40322068095207214\n",
      "  batch 200 loss: 0.3918110041320324\n",
      "LOSS train 0.3918110041320324 valid 1.5169932935386896\n",
      "EPOCH 123:\n",
      "  batch 100 loss: 0.4270127694308758\n",
      "  batch 200 loss: 0.4113935799896717\n",
      "LOSS train 0.4113935799896717 valid 1.5241881776601076\n",
      "EPOCH 124:\n",
      "  batch 100 loss: 0.35380337327718736\n",
      "  batch 200 loss: 0.3926550625264645\n",
      "LOSS train 0.3926550625264645 valid 1.4961172007024288\n",
      "EPOCH 125:\n",
      "  batch 100 loss: 0.3612474866211414\n",
      "  batch 200 loss: 0.35977710530161855\n",
      "LOSS train 0.35977710530161855 valid 1.4864478055387735\n",
      "EPOCH 126:\n",
      "  batch 100 loss: 0.3580635093897581\n",
      "  batch 200 loss: 0.3488717260211706\n",
      "LOSS train 0.3488717260211706 valid 1.500726817175746\n",
      "EPOCH 127:\n",
      "  batch 100 loss: 0.346368653178215\n",
      "  batch 200 loss: 0.3653431026637554\n",
      "LOSS train 0.3653431026637554 valid 1.4865864049643278\n",
      "EPOCH 128:\n",
      "  batch 100 loss: 0.3305923095345497\n",
      "  batch 200 loss: 0.37557545959949495\n",
      "LOSS train 0.37557545959949495 valid 1.5008857119828463\n",
      "EPOCH 129:\n",
      "  batch 100 loss: 0.3533014303445816\n",
      "  batch 200 loss: 0.3553163805603981\n",
      "LOSS train 0.3553163805603981 valid 1.4865001775324345\n",
      "EPOCH 130:\n",
      "  batch 100 loss: 0.3397213468700647\n",
      "  batch 200 loss: 0.34830245189368725\n",
      "LOSS train 0.34830245189368725 valid 1.4685282781720161\n",
      "EPOCH 131:\n",
      "  batch 100 loss: 0.37014410227537153\n",
      "  batch 200 loss: 0.3423770348727703\n",
      "LOSS train 0.3423770348727703 valid 1.472645539790392\n",
      "EPOCH 132:\n",
      "  batch 100 loss: 0.3198538464307785\n",
      "  batch 200 loss: 0.34934386171400544\n",
      "LOSS train 0.34934386171400544 valid 1.4713833294808865\n",
      "EPOCH 133:\n",
      "  batch 100 loss: 0.33229856967926025\n",
      "  batch 200 loss: 0.35078323885798457\n",
      "LOSS train 0.35078323885798457 valid 1.4760355539619923\n",
      "EPOCH 134:\n",
      "  batch 100 loss: 0.34846135824918745\n",
      "  batch 200 loss: 0.34936581656336785\n",
      "LOSS train 0.34936581656336785 valid 1.476973632350564\n",
      "EPOCH 135:\n",
      "  batch 100 loss: 0.32263052843511103\n",
      "  batch 200 loss: 0.3541928769648075\n",
      "LOSS train 0.3541928769648075 valid 1.4625232107937336\n",
      "EPOCH 136:\n",
      "  batch 100 loss: 0.34933575250208376\n",
      "  batch 200 loss: 0.34449607610702515\n",
      "LOSS train 0.34449607610702515 valid 1.4669671412557364\n",
      "EPOCH 137:\n",
      "  batch 100 loss: 0.3078898301720619\n",
      "  batch 200 loss: 0.35069468572735785\n",
      "LOSS train 0.35069468572735785 valid 1.5054203029721975\n",
      "EPOCH 138:\n",
      "  batch 100 loss: 0.31600753679871557\n",
      "  batch 200 loss: 0.3369678217917681\n",
      "LOSS train 0.3369678217917681 valid 1.5007086638361216\n",
      "EPOCH 139:\n",
      "  batch 100 loss: 0.3476491129398346\n",
      "  batch 200 loss: 0.36364081159234046\n",
      "LOSS train 0.36364081159234046 valid 1.5112444451078773\n",
      "EPOCH 140:\n",
      "  batch 100 loss: 0.3246187643706799\n",
      "  batch 200 loss: 0.3310387896001339\n",
      "LOSS train 0.3310387896001339 valid 1.490497287362814\n",
      "EPOCH 141:\n",
      "  batch 100 loss: 0.3260360708832741\n",
      "  batch 200 loss: 0.32853007182478905\n",
      "LOSS train 0.32853007182478905 valid 1.4834504555910826\n",
      "EPOCH 142:\n",
      "  batch 100 loss: 0.31701745733618736\n",
      "  batch 200 loss: 0.29367428362369535\n",
      "LOSS train 0.29367428362369535 valid 1.4867169130593538\n",
      "EPOCH 143:\n",
      "  batch 100 loss: 0.3301669230312109\n",
      "  batch 200 loss: 0.3298680980503559\n",
      "LOSS train 0.3298680980503559 valid 1.5135402865707874\n",
      "EPOCH 144:\n",
      "  batch 100 loss: 0.2887926545739174\n",
      "  batch 200 loss: 0.34097465470433236\n",
      "LOSS train 0.34097465470433236 valid 1.4697975320741534\n",
      "EPOCH 145:\n",
      "  batch 100 loss: 0.3245613169670105\n",
      "  batch 200 loss: 0.3136886928230524\n",
      "LOSS train 0.3136886928230524 valid 1.4797747079283\n",
      "EPOCH 146:\n",
      "  batch 100 loss: 0.317936635017395\n",
      "  batch 200 loss: 0.32467077068984507\n",
      "LOSS train 0.32467077068984507 valid 1.4947757869958878\n",
      "EPOCH 147:\n",
      "  batch 100 loss: 0.27955508649349214\n",
      "  batch 200 loss: 0.32430011600255965\n",
      "LOSS train 0.32430011600255965 valid 1.502508008852601\n",
      "EPOCH 148:\n",
      "  batch 100 loss: 0.289122831299901\n",
      "  batch 200 loss: 0.3128004352748394\n",
      "LOSS train 0.3128004352748394 valid 1.4807782880961895\n",
      "EPOCH 149:\n",
      "  batch 100 loss: 0.30649342119693757\n",
      "  batch 200 loss: 0.2810242659598589\n",
      "LOSS train 0.2810242659598589 valid 1.464541593566537\n",
      "EPOCH 150:\n",
      "  batch 100 loss: 0.28093420758843424\n",
      "  batch 200 loss: 0.3114473056048155\n",
      "LOSS train 0.3114473056048155 valid 1.4752028975635767\n",
      "EPOCH 151:\n",
      "  batch 100 loss: 0.2873283476382494\n",
      "  batch 200 loss: 0.2840899057686329\n",
      "LOSS train 0.2840899057686329 valid 1.486765107139945\n",
      "EPOCH 152:\n",
      "  batch 100 loss: 0.2872475495934486\n",
      "  batch 200 loss: 0.276836351826787\n",
      "LOSS train 0.276836351826787 valid 1.4824959300458431\n",
      "EPOCH 153:\n",
      "  batch 100 loss: 0.2842278651148081\n",
      "  batch 200 loss: 0.30585098907351493\n",
      "LOSS train 0.30585098907351493 valid 1.4626647420227528\n",
      "EPOCH 154:\n",
      "  batch 100 loss: 0.2744890692830086\n",
      "  batch 200 loss: 0.28481272526085377\n",
      "LOSS train 0.28481272526085377 valid 1.4988023415207863\n",
      "EPOCH 155:\n",
      "  batch 100 loss: 0.28046244487166405\n",
      "  batch 200 loss: 0.28755694173276425\n",
      "LOSS train 0.28755694173276425 valid 1.4895040150731802\n",
      "EPOCH 156:\n",
      "  batch 100 loss: 0.2937750495225191\n",
      "  batch 200 loss: 0.28779645025730133\n",
      "LOSS train 0.28779645025730133 valid 1.4580708090215921\n",
      "EPOCH 157:\n",
      "  batch 100 loss: 0.2833770523965359\n",
      "  batch 200 loss: 0.30977787472307683\n",
      "LOSS train 0.30977787472307683 valid 1.5018786117434502\n",
      "EPOCH 158:\n",
      "  batch 100 loss: 0.2784698560833931\n",
      "  batch 200 loss: 0.32345648609101774\n",
      "LOSS train 0.32345648609101774 valid 1.4792568841949105\n",
      "EPOCH 159:\n",
      "  batch 100 loss: 0.28869897067546846\n",
      "  batch 200 loss: 0.2844161307811737\n",
      "LOSS train 0.2844161307811737 valid 1.4622055906802416\n",
      "EPOCH 160:\n",
      "  batch 100 loss: 0.284856563732028\n",
      "  batch 200 loss: 0.2822522772103548\n",
      "LOSS train 0.2822522772103548 valid 1.4696511309593916\n",
      "EPOCH 161:\n",
      "  batch 100 loss: 0.26624625124037266\n",
      "  batch 200 loss: 0.2831978373974562\n",
      "LOSS train 0.2831978373974562 valid 1.4725935021415353\n",
      "EPOCH 162:\n",
      "  batch 100 loss: 0.26998653903603553\n",
      "  batch 200 loss: 0.2944795950502157\n",
      "LOSS train 0.2944795950502157 valid 1.4851662535220385\n",
      "EPOCH 163:\n",
      "  batch 100 loss: 0.28873203236609696\n",
      "  batch 200 loss: 0.28323240727186205\n",
      "LOSS train 0.28323240727186205 valid 1.4900224413722754\n",
      "EPOCH 164:\n",
      "  batch 100 loss: 0.2773151462525129\n",
      "  batch 200 loss: 0.2957912847399712\n",
      "LOSS train 0.2957912847399712 valid 1.467539057135582\n",
      "EPOCH 165:\n",
      "  batch 100 loss: 0.27319263249635695\n",
      "  batch 200 loss: 0.28789626859128475\n",
      "LOSS train 0.28789626859128475 valid 1.464739402756095\n",
      "EPOCH 166:\n",
      "  batch 100 loss: 0.2803830885142088\n",
      "  batch 200 loss: 0.28413127072155475\n",
      "LOSS train 0.28413127072155475 valid 1.4684231840074062\n",
      "EPOCH 167:\n",
      "  batch 100 loss: 0.29816864907741547\n",
      "  batch 200 loss: 0.26641182027757165\n",
      "LOSS train 0.26641182027757165 valid 1.4759917836636305\n",
      "EPOCH 168:\n",
      "  batch 100 loss: 0.2696875827014446\n",
      "  batch 200 loss: 0.26148298613727095\n",
      "LOSS train 0.26148298613727095 valid 1.4677880201488733\n",
      "EPOCH 169:\n",
      "  batch 100 loss: 0.25557548433542254\n",
      "  batch 200 loss: 0.2759474069625139\n",
      "LOSS train 0.2759474069625139 valid 1.464072298258543\n",
      "EPOCH 170:\n",
      "  batch 100 loss: 0.24720428343862294\n",
      "  batch 200 loss: 0.26953403994441033\n",
      "LOSS train 0.26953403994441033 valid 1.471912194043398\n",
      "EPOCH 171:\n",
      "  batch 100 loss: 0.27280590519309045\n",
      "  batch 200 loss: 0.27064580395817756\n",
      "LOSS train 0.27064580395817756 valid 1.4715775772929192\n",
      "EPOCH 172:\n",
      "  batch 100 loss: 0.26968284364789724\n",
      "  batch 200 loss: 0.2555785644054413\n",
      "LOSS train 0.2555785644054413 valid 1.4776398353278637\n",
      "EPOCH 173:\n",
      "  batch 100 loss: 0.2713799412548542\n",
      "  batch 200 loss: 0.2797209506481886\n",
      "LOSS train 0.2797209506481886 valid 1.4707513730973005\n",
      "EPOCH 174:\n",
      "  batch 100 loss: 0.27945867471396924\n",
      "  batch 200 loss: 0.248854808434844\n",
      "LOSS train 0.248854808434844 valid 1.477022735401988\n",
      "EPOCH 175:\n",
      "  batch 100 loss: 0.2540690924972296\n",
      "  batch 200 loss: 0.2643365651369095\n",
      "LOSS train 0.2643365651369095 valid 1.462550638243556\n",
      "EPOCH 176:\n",
      "  batch 100 loss: 0.24246404014527798\n",
      "  batch 200 loss: 0.2804972480982542\n",
      "LOSS train 0.2804972480982542 valid 1.4668510910123587\n",
      "EPOCH 177:\n",
      "  batch 100 loss: 0.26412154130637644\n",
      "  batch 200 loss: 0.26798916347324847\n",
      "LOSS train 0.26798916347324847 valid 1.4633392933756113\n",
      "EPOCH 178:\n",
      "  batch 100 loss: 0.23331148587167264\n",
      "  batch 200 loss: 0.26158142507076265\n",
      "LOSS train 0.26158142507076265 valid 1.476288728415966\n",
      "EPOCH 179:\n",
      "  batch 100 loss: 0.24979130551218987\n",
      "  batch 200 loss: 0.27547335617244245\n",
      "LOSS train 0.27547335617244245 valid 1.4653600454330444\n",
      "EPOCH 180:\n",
      "  batch 100 loss: 0.24227863490581514\n",
      "  batch 200 loss: 0.2797082207351923\n",
      "LOSS train 0.2797082207351923 valid 1.4738311488181353\n",
      "EPOCH 181:\n",
      "  batch 100 loss: 0.24932708010077476\n",
      "  batch 200 loss: 0.249433910548687\n",
      "LOSS train 0.249433910548687 valid 1.477108158171177\n",
      "EPOCH 182:\n",
      "  batch 100 loss: 0.2520325601100922\n",
      "  batch 200 loss: 0.25703569658100606\n",
      "LOSS train 0.25703569658100606 valid 1.4667006693780422\n",
      "EPOCH 183:\n",
      "  batch 100 loss: 0.2620954438298941\n",
      "  batch 200 loss: 0.2676560199260712\n",
      "LOSS train 0.2676560199260712 valid 1.4641792383044958\n",
      "EPOCH 184:\n",
      "  batch 100 loss: 0.265506881326437\n",
      "  batch 200 loss: 0.2431459141522646\n",
      "LOSS train 0.2431459141522646 valid 1.4677895121276379\n",
      "EPOCH 185:\n",
      "  batch 100 loss: 0.2553071039170027\n",
      "  batch 200 loss: 0.25977910175919533\n",
      "LOSS train 0.25977910175919533 valid 1.4663773365318775\n",
      "EPOCH 186:\n",
      "  batch 100 loss: 0.25874503023922446\n",
      "  batch 200 loss: 0.25256194643676283\n",
      "LOSS train 0.25256194643676283 valid 1.4700908921658993\n",
      "EPOCH 187:\n",
      "  batch 100 loss: 0.23505611777305602\n",
      "  batch 200 loss: 0.2579562198370695\n",
      "LOSS train 0.2579562198370695 valid 1.4665792975574732\n",
      "EPOCH 188:\n",
      "  batch 100 loss: 0.28199166715145113\n",
      "  batch 200 loss: 0.24710304014384746\n",
      "LOSS train 0.24710304014384746 valid 1.4741021543741226\n",
      "EPOCH 189:\n",
      "  batch 100 loss: 0.2529388939589262\n",
      "  batch 200 loss: 0.25534704625606536\n",
      "LOSS train 0.25534704625606536 valid 1.4743249211460352\n",
      "EPOCH 190:\n",
      "  batch 100 loss: 0.264129508882761\n",
      "  batch 200 loss: 0.24464518576860428\n",
      "LOSS train 0.24464518576860428 valid 1.4705538488924503\n",
      "EPOCH 191:\n",
      "  batch 100 loss: 0.26360834077000617\n",
      "  batch 200 loss: 0.23869935639202594\n",
      "LOSS train 0.23869935639202594 valid 1.4672136958688498\n",
      "EPOCH 192:\n",
      "  batch 100 loss: 0.24377882786095142\n",
      "  batch 200 loss: 0.25438907854259013\n",
      "LOSS train 0.25438907854259013 valid 1.4727227054536343\n",
      "EPOCH 193:\n",
      "  batch 100 loss: 0.23872905269265174\n",
      "  batch 200 loss: 0.2635435377806425\n",
      "LOSS train 0.2635435377806425 valid 1.4747163755819201\n",
      "EPOCH 194:\n",
      "  batch 100 loss: 0.24240091390907764\n",
      "  batch 200 loss: 0.26184242859482765\n",
      "LOSS train 0.26184242859482765 valid 1.468691066838801\n",
      "EPOCH 195:\n",
      "  batch 100 loss: 0.24348953656852246\n",
      "  batch 200 loss: 0.24471579886972905\n",
      "LOSS train 0.24471579886972905 valid 1.4651607908308506\n",
      "EPOCH 196:\n",
      "  batch 100 loss: 0.24881190720945598\n",
      "  batch 200 loss: 0.24604151226580143\n",
      "LOSS train 0.24604151226580143 valid 1.4821377219632268\n",
      "EPOCH 197:\n",
      "  batch 100 loss: 0.2659007715433836\n",
      "  batch 200 loss: 0.2508483723551035\n",
      "LOSS train 0.2508483723551035 valid 1.4705425202846527\n",
      "EPOCH 198:\n",
      "  batch 100 loss: 0.26655587382614615\n",
      "  batch 200 loss: 0.2406923307850957\n",
      "LOSS train 0.2406923307850957 valid 1.4817207213491201\n",
      "EPOCH 199:\n",
      "  batch 100 loss: 0.24186816677451134\n",
      "  batch 200 loss: 0.23290445756167175\n",
      "LOSS train 0.23290445756167175 valid 1.4764421302825212\n",
      "EPOCH 200:\n",
      "  batch 100 loss: 0.24832298554480076\n",
      "  batch 200 loss: 0.24804475329816342\n",
      "LOSS train 0.24804475329816342 valid 1.466194512322545\n",
      "EPOCH 201:\n",
      "  batch 100 loss: 0.22650128461420535\n",
      "  batch 200 loss: 0.2725513633340597\n",
      "LOSS train 0.2725513633340597 valid 1.4627430448308587\n",
      "EPOCH 202:\n",
      "  batch 100 loss: 0.23481177739799022\n",
      "  batch 200 loss: 0.2517242654040456\n",
      "LOSS train 0.2517242654040456 valid 1.4723149295896292\n",
      "EPOCH 203:\n",
      "  batch 100 loss: 0.22750452034175395\n",
      "  batch 200 loss: 0.24881827436387538\n",
      "LOSS train 0.24881827436387538 valid 1.4656038954854012\n",
      "EPOCH 204:\n",
      "  batch 100 loss: 0.26237548775970937\n",
      "  batch 200 loss: 0.23283352121710776\n",
      "LOSS train 0.23283352121710776 valid 1.4666590001434088\n",
      "EPOCH 205:\n",
      "  batch 100 loss: 0.25082334578037263\n",
      "  batch 200 loss: 0.25020466968417165\n",
      "LOSS train 0.25020466968417165 valid 1.4636074658483267\n",
      "EPOCH 206:\n",
      "  batch 100 loss: 0.23404337547719478\n",
      "  batch 200 loss: 0.242757875174284\n",
      "LOSS train 0.242757875174284 valid 1.468958219513297\n",
      "EPOCH 207:\n",
      "  batch 100 loss: 0.26299179323017596\n",
      "  batch 200 loss: 0.24130013916641474\n",
      "LOSS train 0.24130013916641474 valid 1.466469831764698\n",
      "EPOCH 208:\n",
      "  batch 100 loss: 0.22963304728269576\n",
      "  batch 200 loss: 0.2381993041932583\n",
      "LOSS train 0.2381993041932583 valid 1.4774452354758978\n",
      "EPOCH 209:\n",
      "  batch 100 loss: 0.249517709761858\n",
      "  batch 200 loss: 0.2478905103355646\n",
      "LOSS train 0.2478905103355646 valid 1.4763543587177992\n",
      "EPOCH 210:\n",
      "  batch 100 loss: 0.23280799090862275\n",
      "  batch 200 loss: 0.24870325230062007\n",
      "LOSS train 0.24870325230062007 valid 1.4731559054926038\n",
      "EPOCH 211:\n",
      "  batch 100 loss: 0.24082838296890258\n",
      "  batch 200 loss: 0.26157369688153265\n",
      "LOSS train 0.26157369688153265 valid 1.4688390046358109\n",
      "EPOCH 212:\n",
      "  batch 100 loss: 0.25103683277964595\n",
      "  batch 200 loss: 0.2503254670649767\n",
      "LOSS train 0.2503254670649767 valid 1.4672936759889126\n",
      "EPOCH 213:\n",
      "  batch 100 loss: 0.2365717588365078\n",
      "  batch 200 loss: 0.2465551997721195\n",
      "LOSS train 0.2465551997721195 valid 1.4866237118840218\n",
      "EPOCH 214:\n",
      "  batch 100 loss: 0.25568201135843993\n",
      "  batch 200 loss: 0.24109298668801785\n",
      "LOSS train 0.24109298668801785 valid 1.4707560259848833\n",
      "EPOCH 215:\n",
      "  batch 100 loss: 0.2438029432296753\n",
      "  batch 200 loss: 0.2548427629470825\n",
      "LOSS train 0.2548427629470825 valid 1.4687525182962418\n",
      "EPOCH 216:\n",
      "  batch 100 loss: 0.22751398921012878\n",
      "  batch 200 loss: 0.2670907311886549\n",
      "LOSS train 0.2670907311886549 valid 1.470760554075241\n",
      "EPOCH 217:\n",
      "  batch 100 loss: 0.26422820895910265\n",
      "  batch 200 loss: 0.2388293446600437\n",
      "LOSS train 0.2388293446600437 valid 1.4791949708014727\n",
      "EPOCH 218:\n",
      "  batch 100 loss: 0.23520492598414422\n",
      "  batch 200 loss: 0.2728128719329834\n",
      "LOSS train 0.2728128719329834 valid 1.4683330096304417\n",
      "EPOCH 219:\n",
      "  batch 100 loss: 0.25524811431765554\n",
      "  batch 200 loss: 0.23138033874332906\n",
      "LOSS train 0.23138033874332906 valid 1.4775119703263044\n",
      "EPOCH 220:\n",
      "  batch 100 loss: 0.24009705044329166\n",
      "  batch 200 loss: 0.24306841433048249\n",
      "LOSS train 0.24306841433048249 valid 1.4700346756726503\n",
      "EPOCH 221:\n",
      "  batch 100 loss: 0.24431235000491142\n",
      "  batch 200 loss: 0.24286111906170846\n",
      "LOSS train 0.24286111906170846 valid 1.4693642910569906\n",
      "EPOCH 222:\n",
      "  batch 100 loss: 0.25030876591801643\n",
      "  batch 200 loss: 0.25330624856054784\n",
      "LOSS train 0.25330624856054784 valid 1.465650612488389\n",
      "EPOCH 223:\n",
      "  batch 100 loss: 0.2633681506663561\n",
      "  batch 200 loss: 0.23401527374982833\n",
      "LOSS train 0.23401527374982833 valid 1.4652516171336174\n",
      "EPOCH 224:\n",
      "  batch 100 loss: 0.24444550596177578\n",
      "  batch 200 loss: 0.2505215857923031\n",
      "LOSS train 0.2505215857923031 valid 1.4647396225482225\n",
      "EPOCH 225:\n",
      "  batch 100 loss: 0.23316862240433692\n",
      "  batch 200 loss: 0.2583967735990882\n",
      "LOSS train 0.2583967735990882 valid 1.4671993609517813\n",
      "EPOCH 226:\n",
      "  batch 100 loss: 0.23388729453086854\n",
      "  batch 200 loss: 0.25193015903234484\n",
      "LOSS train 0.25193015903234484 valid 1.4776108544319868\n",
      "EPOCH 227:\n",
      "  batch 100 loss: 0.2565373124927282\n",
      "  batch 200 loss: 0.22549872428178788\n",
      "LOSS train 0.22549872428178788 valid 1.4770544655621052\n",
      "EPOCH 228:\n",
      "  batch 100 loss: 0.27458255462348463\n",
      "  batch 200 loss: 0.2219106797873974\n",
      "LOSS train 0.2219106797873974 valid 1.46917730756104\n",
      "EPOCH 229:\n",
      "  batch 100 loss: 0.234393880777061\n",
      "  batch 200 loss: 0.25357465762645004\n",
      "LOSS train 0.25357465762645004 valid 1.4677712079137564\n",
      "EPOCH 230:\n",
      "  batch 100 loss: 0.2478332994878292\n",
      "  batch 200 loss: 0.2524129869043827\n",
      "LOSS train 0.2524129869043827 valid 1.4683484584093094\n",
      "EPOCH 231:\n",
      "  batch 100 loss: 0.23198963940143585\n",
      "  batch 200 loss: 0.26480812326073644\n",
      "LOSS train 0.26480812326073644 valid 1.473140724003315\n",
      "EPOCH 232:\n",
      "  batch 100 loss: 0.2603620526194572\n",
      "  batch 200 loss: 0.24775293074548244\n",
      "LOSS train 0.24775293074548244 valid 1.4727948158979416\n",
      "EPOCH 233:\n",
      "  batch 100 loss: 0.23851511601358652\n",
      "  batch 200 loss: 0.2520503210276365\n",
      "LOSS train 0.2520503210276365 valid 1.4677611514925957\n",
      "EPOCH 234:\n",
      "  batch 100 loss: 0.24183155737817288\n",
      "  batch 200 loss: 0.23258948501199483\n",
      "LOSS train 0.23258948501199483 valid 1.4696448594331741\n",
      "EPOCH 235:\n",
      "  batch 100 loss: 0.26703238509595395\n",
      "  batch 200 loss: 0.23599839732050895\n",
      "LOSS train 0.23599839732050895 valid 1.4700300954282284\n",
      "EPOCH 236:\n",
      "  batch 100 loss: 0.26505084447562693\n",
      "  batch 200 loss: 0.22279433332383633\n",
      "LOSS train 0.22279433332383633 valid 1.4665959030389786\n",
      "EPOCH 237:\n",
      "  batch 100 loss: 0.24783441133797168\n",
      "  batch 200 loss: 0.24239080265164376\n",
      "LOSS train 0.24239080265164376 valid 1.4658287316560745\n",
      "EPOCH 238:\n",
      "  batch 100 loss: 0.24188506461679934\n",
      "  batch 200 loss: 0.23775285445153713\n",
      "LOSS train 0.23775285445153713 valid 1.4717143177986145\n",
      "EPOCH 239:\n",
      "  batch 100 loss: 0.24320805460214615\n",
      "  batch 200 loss: 0.24886083081364632\n",
      "LOSS train 0.24886083081364632 valid 1.4878235245123506\n",
      "EPOCH 240:\n",
      "  batch 100 loss: 0.23184733994305134\n",
      "  batch 200 loss: 0.23880213916301726\n",
      "LOSS train 0.23880213916301726 valid 1.4658050574362278\n",
      "EPOCH 241:\n",
      "  batch 100 loss: 0.24138300571590662\n",
      "  batch 200 loss: 0.2592827024310827\n",
      "LOSS train 0.2592827024310827 valid 1.4706194205209613\n",
      "EPOCH 242:\n",
      "  batch 100 loss: 0.23675917413085698\n",
      "  batch 200 loss: 0.238074856325984\n",
      "LOSS train 0.238074856325984 valid 1.4680109564214945\n",
      "EPOCH 243:\n",
      "  batch 100 loss: 0.22561299666762352\n",
      "  batch 200 loss: 0.27538944080471994\n",
      "LOSS train 0.27538944080471994 valid 1.4668604899197817\n",
      "EPOCH 244:\n",
      "  batch 100 loss: 0.25054481521248817\n",
      "  batch 200 loss: 0.24864702470600605\n",
      "LOSS train 0.24864702470600605 valid 1.4652778822928667\n",
      "EPOCH 245:\n",
      "  batch 100 loss: 0.24660383649170398\n",
      "  batch 200 loss: 0.2419107087701559\n",
      "LOSS train 0.2419107087701559 valid 1.4650947842746973\n",
      "EPOCH 246:\n",
      "  batch 100 loss: 0.24837687775492667\n",
      "  batch 200 loss: 0.2376424575224519\n",
      "LOSS train 0.2376424575224519 valid 1.4818469174206257\n",
      "EPOCH 247:\n",
      "  batch 100 loss: 0.2412605332583189\n",
      "  batch 200 loss: 0.24173682488501072\n",
      "LOSS train 0.24173682488501072 valid 1.4690116867423058\n",
      "EPOCH 248:\n",
      "  batch 100 loss: 0.2422734224051237\n",
      "  batch 200 loss: 0.2417739699780941\n",
      "LOSS train 0.2417739699780941 valid 1.466602886095643\n",
      "EPOCH 249:\n",
      "  batch 100 loss: 0.24539553098380565\n",
      "  batch 200 loss: 0.24501273937523366\n",
      "LOSS train 0.24501273937523366 valid 1.4773161746561527\n",
      "EPOCH 250:\n",
      "  batch 100 loss: 0.2573622803762555\n",
      "  batch 200 loss: 0.25306088097393514\n",
      "LOSS train 0.25306088097393514 valid 1.469781806692481\n",
      "EPOCH 251:\n",
      "  batch 100 loss: 0.24333266839385032\n",
      "  batch 200 loss: 0.24681634694337845\n",
      "LOSS train 0.24681634694337845 valid 1.472162215039134\n",
      "EPOCH 252:\n",
      "  batch 100 loss: 0.24259259343147277\n",
      "  batch 200 loss: 0.25465630888938906\n",
      "LOSS train 0.25465630888938906 valid 1.4729620134457946\n",
      "EPOCH 253:\n",
      "  batch 100 loss: 0.25101512774825097\n",
      "  batch 200 loss: 0.23474020898342132\n",
      "LOSS train 0.23474020898342132 valid 1.4667130336165428\n",
      "EPOCH 254:\n",
      "  batch 100 loss: 0.2450396003574133\n",
      "  batch 200 loss: 0.2525050488114357\n",
      "LOSS train 0.2525050488114357 valid 1.4712137756869197\n",
      "EPOCH 255:\n",
      "  batch 100 loss: 0.2378143035620451\n",
      "  batch 200 loss: 0.2429171334952116\n",
      "LOSS train 0.2429171334952116 valid 1.465358817949891\n",
      "EPOCH 256:\n",
      "  batch 100 loss: 0.24593664966523648\n",
      "  batch 200 loss: 0.2468958558887243\n",
      "LOSS train 0.2468958558887243 valid 1.4634866518899798\n",
      "EPOCH 257:\n",
      "  batch 100 loss: 0.2564738381654024\n",
      "  batch 200 loss: 0.23705597463995218\n",
      "LOSS train 0.23705597463995218 valid 1.4662156328558922\n",
      "EPOCH 258:\n",
      "  batch 100 loss: 0.251395523250103\n",
      "  batch 200 loss: 0.241046162545681\n",
      "LOSS train 0.241046162545681 valid 1.4765609335154295\n",
      "EPOCH 259:\n",
      "  batch 100 loss: 0.23767686776816846\n",
      "  batch 200 loss: 0.24581932537257672\n",
      "LOSS train 0.24581932537257672 valid 1.4648164426907897\n",
      "EPOCH 260:\n",
      "  batch 100 loss: 0.25796745881438254\n",
      "  batch 200 loss: 0.2451488670706749\n",
      "LOSS train 0.2451488670706749 valid 1.4676028294488788\n",
      "EPOCH 261:\n",
      "  batch 100 loss: 0.2538335879147053\n",
      "  batch 200 loss: 0.23321701519191265\n",
      "LOSS train 0.23321701519191265 valid 1.4650355000048876\n",
      "EPOCH 262:\n",
      "  batch 100 loss: 0.250191644616425\n",
      "  batch 200 loss: 0.23443135268986226\n",
      "LOSS train 0.23443135268986226 valid 1.4672280931845307\n",
      "EPOCH 263:\n",
      "  batch 100 loss: 0.2612864762544632\n",
      "  batch 200 loss: 0.23474340319633483\n",
      "LOSS train 0.23474340319633483 valid 1.4636302758008242\n",
      "EPOCH 264:\n",
      "  batch 100 loss: 0.23459328524768353\n",
      "  batch 200 loss: 0.24584714576601982\n",
      "LOSS train 0.24584714576601982 valid 1.4665210265666246\n",
      "EPOCH 265:\n",
      "  batch 100 loss: 0.25532097183167934\n",
      "  batch 200 loss: 0.25412380561232567\n",
      "LOSS train 0.25412380561232567 valid 1.4657535022124648\n",
      "EPOCH 266:\n",
      "  batch 100 loss: 0.25975431136786936\n",
      "  batch 200 loss: 0.24042247146368026\n",
      "LOSS train 0.24042247146368026 valid 1.4642321486026049\n",
      "EPOCH 267:\n",
      "  batch 100 loss: 0.23636144757270813\n",
      "  batch 200 loss: 0.23624808005988598\n",
      "LOSS train 0.23624808005988598 valid 1.4680696101859212\n",
      "EPOCH 268:\n",
      "  batch 100 loss: 0.2600921327620745\n",
      "  batch 200 loss: 0.22716803923249246\n",
      "LOSS train 0.22716803923249246 valid 1.476310620084405\n",
      "EPOCH 269:\n",
      "  batch 100 loss: 0.23481032446026803\n",
      "  batch 200 loss: 0.27035625524818896\n",
      "LOSS train 0.27035625524818896 valid 1.4670225847512484\n",
      "EPOCH 270:\n",
      "  batch 100 loss: 0.2444931797683239\n",
      "  batch 200 loss: 0.24602669768035412\n",
      "LOSS train 0.24602669768035412 valid 1.468959509395063\n",
      "EPOCH 271:\n",
      "  batch 100 loss: 0.24458908312022687\n",
      "  batch 200 loss: 0.2461151782423258\n",
      "LOSS train 0.2461151782423258 valid 1.4691881146281958\n",
      "EPOCH 272:\n",
      "  batch 100 loss: 0.2377574263513088\n",
      "  batch 200 loss: 0.242887150272727\n",
      "LOSS train 0.242887150272727 valid 1.4687456768006086\n",
      "EPOCH 273:\n",
      "  batch 100 loss: 0.2479215955734253\n",
      "  batch 200 loss: 0.24656611997634173\n",
      "LOSS train 0.24656611997634173 valid 1.4740467993542552\n",
      "EPOCH 274:\n",
      "  batch 100 loss: 0.23770340010523797\n",
      "  batch 200 loss: 0.23298262622207402\n",
      "LOSS train 0.23298262622207402 valid 1.469555415213108\n",
      "EPOCH 275:\n",
      "  batch 100 loss: 0.23193817730993033\n",
      "  batch 200 loss: 0.2635449574142694\n",
      "LOSS train 0.2635449574142694 valid 1.476256676018238\n",
      "EPOCH 276:\n",
      "  batch 100 loss: 0.25286359801888464\n",
      "  batch 200 loss: 0.24238658353686332\n",
      "LOSS train 0.24238658353686332 valid 1.4710245747119188\n",
      "EPOCH 277:\n",
      "  batch 100 loss: 0.22681346967816352\n",
      "  batch 200 loss: 0.25383694633841514\n",
      "LOSS train 0.25383694633841514 valid 1.4689922593533993\n",
      "EPOCH 278:\n",
      "  batch 100 loss: 0.23995540007948876\n",
      "  batch 200 loss: 0.24308243095874787\n",
      "LOSS train 0.24308243095874787 valid 1.4716949760913849\n",
      "EPOCH 279:\n",
      "  batch 100 loss: 0.22834366634488107\n",
      "  batch 200 loss: 0.2449969371408224\n",
      "LOSS train 0.2449969371408224 valid 1.474792405962944\n",
      "EPOCH 280:\n",
      "  batch 100 loss: 0.24592819295823573\n",
      "  batch 200 loss: 0.2367614422738552\n",
      "LOSS train 0.2367614422738552 valid 1.4755884697660804\n",
      "EPOCH 281:\n",
      "  batch 100 loss: 0.26875138461589815\n",
      "  batch 200 loss: 0.2251444659382105\n",
      "LOSS train 0.2251444659382105 valid 1.4722383636981249\n",
      "EPOCH 282:\n",
      "  batch 100 loss: 0.2395589217171073\n",
      "  batch 200 loss: 0.2400955718755722\n",
      "LOSS train 0.2400955718755722 valid 1.465850466862321\n",
      "EPOCH 283:\n",
      "  batch 100 loss: 0.25399452954530716\n",
      "  batch 200 loss: 0.23262843020260335\n",
      "LOSS train 0.23262843020260335 valid 1.4625988686457276\n",
      "EPOCH 284:\n",
      "  batch 100 loss: 0.2715847334265709\n",
      "  batch 200 loss: 0.22376366034150125\n",
      "LOSS train 0.22376366034150125 valid 1.466840130276978\n",
      "EPOCH 285:\n",
      "  batch 100 loss: 0.24431097149848938\n",
      "  batch 200 loss: 0.25220214277505876\n",
      "LOSS train 0.25220214277505876 valid 1.4721260126680136\n",
      "EPOCH 286:\n",
      "  batch 100 loss: 0.2595033723860979\n",
      "  batch 200 loss: 0.22737424895167352\n",
      "LOSS train 0.22737424895167352 valid 1.4662784337997437\n",
      "EPOCH 287:\n",
      "  batch 100 loss: 0.26027715146541597\n",
      "  batch 200 loss: 0.240066389515996\n",
      "LOSS train 0.240066389515996 valid 1.4665031768381596\n",
      "EPOCH 288:\n",
      "  batch 100 loss: 0.2634220250695944\n",
      "  batch 200 loss: 0.22979605108499526\n",
      "LOSS train 0.22979605108499526 valid 1.4650691617280245\n",
      "EPOCH 289:\n",
      "  batch 100 loss: 0.24536905132234096\n",
      "  batch 200 loss: 0.2391451995074749\n",
      "LOSS train 0.2391451995074749 valid 1.4731763862073421\n",
      "EPOCH 290:\n",
      "  batch 100 loss: 0.2554389755427837\n",
      "  batch 200 loss: 0.22676044754683972\n",
      "LOSS train 0.22676044754683972 valid 1.462627001106739\n",
      "EPOCH 291:\n",
      "  batch 100 loss: 0.2407082075625658\n",
      "  batch 200 loss: 0.23883155681192875\n",
      "LOSS train 0.23883155681192875 valid 1.4738310109823942\n",
      "EPOCH 292:\n",
      "  batch 100 loss: 0.24015473276376725\n",
      "  batch 200 loss: 0.2638783073425293\n",
      "LOSS train 0.2638783073425293 valid 1.480375080369413\n",
      "EPOCH 293:\n",
      "  batch 100 loss: 0.26016122817993165\n",
      "  batch 200 loss: 0.23086063824594022\n",
      "LOSS train 0.23086063824594022 valid 1.472959111444652\n",
      "EPOCH 294:\n",
      "  batch 100 loss: 0.22816374342888593\n",
      "  batch 200 loss: 0.2658883795887232\n",
      "LOSS train 0.2658883795887232 valid 1.4759418219327927\n",
      "EPOCH 295:\n",
      "  batch 100 loss: 0.2650357791781425\n",
      "  batch 200 loss: 0.23183282427489757\n",
      "LOSS train 0.23183282427489757 valid 1.4675576854497194\n",
      "EPOCH 296:\n",
      "  batch 100 loss: 0.23170009449124337\n",
      "  batch 200 loss: 0.252295353487134\n",
      "LOSS train 0.252295353487134 valid 1.4726673029363155\n",
      "EPOCH 297:\n",
      "  batch 100 loss: 0.2393953414261341\n",
      "  batch 200 loss: 0.25304295554757117\n",
      "LOSS train 0.25304295554757117 valid 1.4717325903475285\n",
      "EPOCH 298:\n",
      "  batch 100 loss: 0.22962514504790307\n",
      "  batch 200 loss: 0.25895824916660787\n",
      "LOSS train 0.25895824916660787 valid 1.4627915555611253\n",
      "EPOCH 299:\n",
      "  batch 100 loss: 0.24597457844763995\n",
      "  batch 200 loss: 0.235585453286767\n",
      "LOSS train 0.235585453286767 valid 1.4720932971686125\n",
      "EPOCH 300:\n",
      "  batch 100 loss: 0.2545564207434654\n",
      "  batch 200 loss: 0.2446049790084362\n",
      "LOSS train 0.2446049790084362 valid 1.4844648195430636\n",
      "EPOCH 301:\n",
      "  batch 100 loss: 0.24713065415620805\n",
      "  batch 200 loss: 0.23711395151913167\n",
      "LOSS train 0.23711395151913167 valid 1.4737022295594215\n",
      "EPOCH 302:\n",
      "  batch 100 loss: 0.2439692061394453\n",
      "  batch 200 loss: 0.2521891409158707\n",
      "LOSS train 0.2521891409158707 valid 1.4759105406701565\n",
      "EPOCH 303:\n",
      "  batch 100 loss: 0.2134635117650032\n",
      "  batch 200 loss: 0.2621006055921316\n",
      "LOSS train 0.2621006055921316 valid 1.4757945407181978\n",
      "EPOCH 304:\n",
      "  batch 100 loss: 0.24907408885657786\n",
      "  batch 200 loss: 0.2558301191776991\n",
      "LOSS train 0.2558301191776991 valid 1.4641610160470009\n",
      "EPOCH 305:\n",
      "  batch 100 loss: 0.24672095354646445\n",
      "  batch 200 loss: 0.24458937235176564\n",
      "LOSS train 0.24458937235176564 valid 1.4665853213518858\n",
      "EPOCH 306:\n",
      "  batch 100 loss: 0.2274159261584282\n",
      "  batch 200 loss: 0.2539195046573877\n",
      "LOSS train 0.2539195046573877 valid 1.4680975005030632\n",
      "EPOCH 307:\n",
      "  batch 100 loss: 0.23657379053533079\n",
      "  batch 200 loss: 0.2471201941370964\n",
      "LOSS train 0.2471201941370964 valid 1.472763054072857\n",
      "EPOCH 308:\n",
      "  batch 100 loss: 0.2502230592817068\n",
      "  batch 200 loss: 0.23830603256821634\n",
      "LOSS train 0.23830603256821634 valid 1.483196851797402\n",
      "EPOCH 309:\n",
      "  batch 100 loss: 0.26955080855637786\n",
      "  batch 200 loss: 0.23347935404628514\n",
      "LOSS train 0.23347935404628514 valid 1.4637969266623259\n",
      "EPOCH 310:\n",
      "  batch 100 loss: 0.22478254675865172\n",
      "  batch 200 loss: 0.26106655403971674\n",
      "LOSS train 0.26106655403971674 valid 1.4650545585900545\n",
      "EPOCH 311:\n",
      "  batch 100 loss: 0.24111604198813438\n",
      "  batch 200 loss: 0.26301582396030426\n",
      "LOSS train 0.26301582396030426 valid 1.4689449593424797\n",
      "EPOCH 312:\n",
      "  batch 100 loss: 0.2500985487177968\n",
      "  batch 200 loss: 0.24992607451975346\n",
      "LOSS train 0.24992607451975346 valid 1.469667686149478\n",
      "EPOCH 313:\n",
      "  batch 100 loss: 0.2349784456938505\n",
      "  batch 200 loss: 0.2573398031666875\n",
      "LOSS train 0.2573398031666875 valid 1.4791960781440139\n",
      "EPOCH 314:\n",
      "  batch 100 loss: 0.2499248218536377\n",
      "  batch 200 loss: 0.2517455413937569\n",
      "LOSS train 0.2517455413937569 valid 1.4636769080534577\n",
      "EPOCH 315:\n",
      "  batch 100 loss: 0.2562827631086111\n",
      "  batch 200 loss: 0.2332661225646734\n",
      "LOSS train 0.2332661225646734 valid 1.4690418187528849\n",
      "EPOCH 316:\n",
      "  batch 100 loss: 0.2644227955490351\n",
      "  batch 200 loss: 0.23116068713366986\n",
      "LOSS train 0.23116068713366986 valid 1.4756503775715828\n",
      "EPOCH 317:\n",
      "  batch 100 loss: 0.22776856936514378\n",
      "  batch 200 loss: 0.24555609211325646\n",
      "LOSS train 0.24555609211325646 valid 1.4764173058792949\n",
      "EPOCH 318:\n",
      "  batch 100 loss: 0.2472415627539158\n",
      "  batch 200 loss: 0.2419435528665781\n",
      "LOSS train 0.2419435528665781 valid 1.464526541531086\n",
      "EPOCH 319:\n",
      "  batch 100 loss: 0.2685611380636692\n",
      "  batch 200 loss: 0.21840760238468648\n",
      "LOSS train 0.21840760238468648 valid 1.4690139219164848\n",
      "EPOCH 320:\n",
      "  batch 100 loss: 0.23453713528811931\n",
      "  batch 200 loss: 0.25674028471112254\n",
      "LOSS train 0.25674028471112254 valid 1.4672610238194466\n",
      "EPOCH 321:\n",
      "  batch 100 loss: 0.2241235202550888\n",
      "  batch 200 loss: 0.2505056391656399\n",
      "LOSS train 0.2505056391656399 valid 1.4680897761136293\n",
      "EPOCH 322:\n",
      "  batch 100 loss: 0.23006019078195095\n",
      "  batch 200 loss: 0.25521798402071\n",
      "LOSS train 0.25521798402071 valid 1.48003832064569\n",
      "EPOCH 323:\n",
      "  batch 100 loss: 0.24848486714065074\n",
      "  batch 200 loss: 0.24057291563600303\n",
      "LOSS train 0.24057291563600303 valid 1.467750284820795\n",
      "EPOCH 324:\n",
      "  batch 100 loss: 0.2656169418245554\n",
      "  batch 200 loss: 0.231458241045475\n",
      "LOSS train 0.231458241045475 valid 1.4714108314365149\n",
      "EPOCH 325:\n",
      "  batch 100 loss: 0.24218681190162897\n",
      "  batch 200 loss: 0.2550615411251783\n",
      "LOSS train 0.2550615411251783 valid 1.4693839568644762\n",
      "EPOCH 326:\n",
      "  batch 100 loss: 0.24476417139172554\n",
      "  batch 200 loss: 0.252056841775775\n",
      "LOSS train 0.252056841775775 valid 1.4701381381601095\n",
      "EPOCH 327:\n",
      "  batch 100 loss: 0.23467579677700998\n",
      "  batch 200 loss: 0.2644706283509731\n",
      "LOSS train 0.2644706283509731 valid 1.4869113881140947\n",
      "EPOCH 328:\n",
      "  batch 100 loss: 0.25301362611353395\n",
      "  batch 200 loss: 0.24917038708925246\n",
      "LOSS train 0.24917038708925246 valid 1.467766335234046\n",
      "EPOCH 329:\n",
      "  batch 100 loss: 0.2523087488859892\n",
      "  batch 200 loss: 0.23992754258215426\n",
      "LOSS train 0.23992754258215426 valid 1.477067094296217\n",
      "EPOCH 330:\n",
      "  batch 100 loss: 0.24945813484489918\n",
      "  batch 200 loss: 0.24244800586253404\n",
      "LOSS train 0.24244800586253404 valid 1.4748097136616707\n",
      "EPOCH 331:\n",
      "  batch 100 loss: 0.2433713547885418\n",
      "  batch 200 loss: 0.24420062087476255\n",
      "LOSS train 0.24420062087476255 valid 1.4748273454606533\n",
      "EPOCH 332:\n",
      "  batch 100 loss: 0.2377344162389636\n",
      "  batch 200 loss: 0.2412190443277359\n",
      "LOSS train 0.2412190443277359 valid 1.466349883005023\n",
      "EPOCH 333:\n",
      "  batch 100 loss: 0.2507372758537531\n",
      "  batch 200 loss: 0.2494944915920496\n",
      "LOSS train 0.2494944915920496 valid 1.4644797947257757\n",
      "EPOCH 334:\n",
      "  batch 100 loss: 0.24186397574841975\n",
      "  batch 200 loss: 0.23749831430613993\n",
      "LOSS train 0.23749831430613993 valid 1.4935665261000395\n",
      "EPOCH 335:\n",
      "  batch 100 loss: 0.2488098806887865\n",
      "  batch 200 loss: 0.24969591274857522\n",
      "LOSS train 0.24969591274857522 valid 1.4725373182445765\n",
      "EPOCH 336:\n",
      "  batch 100 loss: 0.2451853223145008\n",
      "  batch 200 loss: 0.24536237314343454\n",
      "LOSS train 0.24536237314343454 valid 1.470253954641521\n",
      "EPOCH 337:\n",
      "  batch 100 loss: 0.25484133064746856\n",
      "  batch 200 loss: 0.2410783564299345\n",
      "LOSS train 0.2410783564299345 valid 1.4755697520449758\n",
      "EPOCH 338:\n",
      "  batch 100 loss: 0.22786339923739432\n",
      "  batch 200 loss: 0.27901318088173865\n",
      "LOSS train 0.27901318088173865 valid 1.4653217270970345\n",
      "EPOCH 339:\n",
      "  batch 100 loss: 0.23683264262974263\n",
      "  batch 200 loss: 0.250531764999032\n",
      "LOSS train 0.250531764999032 valid 1.4670353066176176\n",
      "EPOCH 340:\n",
      "  batch 100 loss: 0.25366860069334507\n",
      "  batch 200 loss: 0.23844150543212891\n",
      "LOSS train 0.23844150543212891 valid 1.4675313467159867\n",
      "EPOCH 341:\n",
      "  batch 100 loss: 0.23819501675665378\n",
      "  batch 200 loss: 0.25054827742278574\n",
      "LOSS train 0.25054827742278574 valid 1.4691707417368889\n",
      "EPOCH 342:\n",
      "  batch 100 loss: 0.24695593774318694\n",
      "  batch 200 loss: 0.2406632112711668\n",
      "LOSS train 0.2406632112711668 valid 1.4756427127867937\n",
      "EPOCH 343:\n",
      "  batch 100 loss: 0.25353743948042395\n",
      "  batch 200 loss: 0.232328579723835\n",
      "LOSS train 0.232328579723835 valid 1.4715985134243965\n",
      "EPOCH 344:\n",
      "  batch 100 loss: 0.2482541400939226\n",
      "  batch 200 loss: 0.24814250946044922\n",
      "LOSS train 0.24814250946044922 valid 1.4691257160156965\n",
      "EPOCH 345:\n",
      "  batch 100 loss: 0.2667640756070614\n",
      "  batch 200 loss: 0.22773523092269898\n",
      "LOSS train 0.22773523092269898 valid 1.4726975448429585\n",
      "EPOCH 346:\n",
      "  batch 100 loss: 0.2247810587659478\n",
      "  batch 200 loss: 0.24117860954254866\n",
      "LOSS train 0.24117860954254866 valid 1.4679295904934406\n",
      "EPOCH 347:\n",
      "  batch 100 loss: 0.2532502692192793\n",
      "  batch 200 loss: 0.23460807360708713\n",
      "LOSS train 0.23460807360708713 valid 1.4805693570524454\n",
      "EPOCH 348:\n",
      "  batch 100 loss: 0.2495870514214039\n",
      "  batch 200 loss: 0.22854764454066753\n",
      "LOSS train 0.22854764454066753 valid 1.4787393659353256\n",
      "EPOCH 349:\n",
      "  batch 100 loss: 0.23004220686852933\n",
      "  batch 200 loss: 0.27299041651189326\n",
      "LOSS train 0.27299041651189326 valid 1.4693451058119535\n",
      "EPOCH 350:\n",
      "  batch 100 loss: 0.24500472240149976\n",
      "  batch 200 loss: 0.2509337802231312\n",
      "LOSS train 0.2509337802231312 valid 1.4713490884751081\n",
      "EPOCH 351:\n",
      "  batch 100 loss: 0.23602334953844548\n",
      "  batch 200 loss: 0.24642931174486876\n",
      "LOSS train 0.24642931174486876 valid 1.4717642031610012\n",
      "EPOCH 352:\n",
      "  batch 100 loss: 0.21875309620052577\n",
      "  batch 200 loss: 0.2539129950851202\n",
      "LOSS train 0.2539129950851202 valid 1.4649564176797867\n",
      "EPOCH 353:\n",
      "  batch 100 loss: 0.2491122793406248\n",
      "  batch 200 loss: 0.24895485006272794\n",
      "LOSS train 0.24895485006272794 valid 1.4774211924523115\n",
      "EPOCH 354:\n",
      "  batch 100 loss: 0.23332726553082467\n",
      "  batch 200 loss: 0.24640837907791138\n",
      "LOSS train 0.24640837907791138 valid 1.465342065319419\n",
      "EPOCH 355:\n",
      "  batch 100 loss: 0.2335923120751977\n",
      "  batch 200 loss: 0.2352196430414915\n",
      "LOSS train 0.2352196430414915 valid 1.4645036440342665\n",
      "EPOCH 356:\n",
      "  batch 100 loss: 0.2513256473094225\n",
      "  batch 200 loss: 0.23527577877044678\n",
      "LOSS train 0.23527577877044678 valid 1.4685305282473564\n",
      "EPOCH 357:\n",
      "  batch 100 loss: 0.23961204260587693\n",
      "  batch 200 loss: 0.24995801843702792\n",
      "LOSS train 0.24995801843702792 valid 1.4692672956734896\n",
      "EPOCH 358:\n",
      "  batch 100 loss: 0.2396498914062977\n",
      "  batch 200 loss: 0.24905415453016758\n",
      "LOSS train 0.24905415453016758 valid 1.4732036776840687\n",
      "EPOCH 359:\n",
      "  batch 100 loss: 0.2374101433902979\n",
      "  batch 200 loss: 0.25359717443585394\n",
      "LOSS train 0.25359717443585394 valid 1.4654544573277235\n",
      "EPOCH 360:\n",
      "  batch 100 loss: 0.24163813054561614\n",
      "  batch 200 loss: 0.2348933757469058\n",
      "LOSS train 0.2348933757469058 valid 1.465830024331808\n",
      "EPOCH 361:\n",
      "  batch 100 loss: 0.2576404628157616\n",
      "  batch 200 loss: 0.23761388808488845\n",
      "LOSS train 0.23761388808488845 valid 1.4795818589627743\n",
      "EPOCH 362:\n",
      "  batch 100 loss: 0.2544236981868744\n",
      "  batch 200 loss: 0.25104358568787577\n",
      "LOSS train 0.25104358568787577 valid 1.47927875071764\n",
      "EPOCH 363:\n",
      "  batch 100 loss: 0.22463810041546822\n",
      "  batch 200 loss: 0.263397149220109\n",
      "LOSS train 0.263397149220109 valid 1.472199235111475\n",
      "EPOCH 364:\n",
      "  batch 100 loss: 0.2584992426633835\n",
      "  batch 200 loss: 0.24312592424452306\n",
      "LOSS train 0.24312592424452306 valid 1.4713392611593008\n",
      "EPOCH 365:\n",
      "  batch 100 loss: 0.2393107672780752\n",
      "  batch 200 loss: 0.2558299541473389\n",
      "LOSS train 0.2558299541473389 valid 1.463713962584734\n",
      "EPOCH 366:\n",
      "  batch 100 loss: 0.23582301110029222\n",
      "  batch 200 loss: 0.2431219904869795\n",
      "LOSS train 0.2431219904869795 valid 1.4659236874431372\n",
      "EPOCH 367:\n",
      "  batch 100 loss: 0.2556941405683756\n",
      "  batch 200 loss: 0.23911774698644878\n",
      "LOSS train 0.23911774698644878 valid 1.464750725775957\n",
      "EPOCH 368:\n",
      "  batch 100 loss: 0.24153947837650777\n",
      "  batch 200 loss: 0.24919071346521376\n",
      "LOSS train 0.24919071346521376 valid 1.4715811517089605\n",
      "EPOCH 369:\n",
      "  batch 100 loss: 0.25451321676373484\n",
      "  batch 200 loss: 0.2427530825138092\n",
      "LOSS train 0.2427530825138092 valid 1.4731448274105787\n",
      "EPOCH 370:\n",
      "  batch 100 loss: 0.2500785604864359\n",
      "  batch 200 loss: 0.2515518406778574\n",
      "LOSS train 0.2515518406778574 valid 1.4736690428107977\n",
      "EPOCH 371:\n",
      "  batch 100 loss: 0.23533205546438693\n",
      "  batch 200 loss: 0.24153583452105523\n",
      "LOSS train 0.24153583452105523 valid 1.4727627532556653\n",
      "EPOCH 372:\n",
      "  batch 100 loss: 0.24941079385578632\n",
      "  batch 200 loss: 0.23681733444333075\n",
      "LOSS train 0.23681733444333075 valid 1.4875832460820675\n",
      "EPOCH 373:\n",
      "  batch 100 loss: 0.23807126723229885\n",
      "  batch 200 loss: 0.2586077228561044\n",
      "LOSS train 0.2586077228561044 valid 1.4827366825193167\n",
      "EPOCH 374:\n",
      "  batch 100 loss: 0.2506063912063837\n",
      "  batch 200 loss: 0.22987944081425668\n",
      "LOSS train 0.22987944081425668 valid 1.4666283149272203\n",
      "EPOCH 375:\n",
      "  batch 100 loss: 0.23364343974739313\n",
      "  batch 200 loss: 0.24539776884019374\n",
      "LOSS train 0.24539776884019374 valid 1.4680361524224281\n",
      "EPOCH 376:\n",
      "  batch 100 loss: 0.23975310906767844\n",
      "  batch 200 loss: 0.25729543320834636\n",
      "LOSS train 0.25729543320834636 valid 1.4630198255181313\n",
      "EPOCH 377:\n",
      "  batch 100 loss: 0.24277344837784767\n",
      "  batch 200 loss: 0.25005399629473685\n",
      "LOSS train 0.25005399629473685 valid 1.4654994253069162\n",
      "EPOCH 378:\n",
      "  batch 100 loss: 0.24951045624911786\n",
      "  batch 200 loss: 0.23460257068276405\n",
      "LOSS train 0.23460257068276405 valid 1.4776544254273176\n",
      "EPOCH 379:\n",
      "  batch 100 loss: 0.2401106856390834\n",
      "  batch 200 loss: 0.2577837660163641\n",
      "LOSS train 0.2577837660163641 valid 1.4757689759135246\n",
      "EPOCH 380:\n",
      "  batch 100 loss: 0.21923830911517142\n",
      "  batch 200 loss: 0.2685856182128191\n",
      "LOSS train 0.2685856182128191 valid 1.4648822117596865\n",
      "EPOCH 381:\n",
      "  batch 100 loss: 0.24971475891768932\n",
      "  batch 200 loss: 0.23390661872923374\n",
      "LOSS train 0.23390661872923374 valid 1.4739156477153301\n",
      "EPOCH 382:\n",
      "  batch 100 loss: 0.24630580708384514\n",
      "  batch 200 loss: 0.24246689654886722\n",
      "LOSS train 0.24246689654886722 valid 1.4757470320910215\n",
      "EPOCH 383:\n",
      "  batch 100 loss: 0.23989388708025217\n",
      "  batch 200 loss: 0.24914366327226162\n",
      "LOSS train 0.24914366327226162 valid 1.4645396955311298\n",
      "EPOCH 384:\n",
      "  batch 100 loss: 0.24203659705817698\n",
      "  batch 200 loss: 0.2486609620600939\n",
      "LOSS train 0.2486609620600939 valid 1.4659074619412422\n",
      "EPOCH 385:\n",
      "  batch 100 loss: 0.23878553457558155\n",
      "  batch 200 loss: 0.2424128545820713\n",
      "LOSS train 0.2424128545820713 valid 1.4691210612654686\n",
      "EPOCH 386:\n",
      "  batch 100 loss: 0.23799011170864104\n",
      "  batch 200 loss: 0.2385852551460266\n",
      "LOSS train 0.2385852551460266 valid 1.4666553307324648\n",
      "EPOCH 387:\n",
      "  batch 100 loss: 0.2563267881423235\n",
      "  batch 200 loss: 0.24530621662735938\n",
      "LOSS train 0.24530621662735938 valid 1.4693570993840694\n",
      "EPOCH 388:\n",
      "  batch 100 loss: 0.2249379114434123\n",
      "  batch 200 loss: 0.25874778665602205\n",
      "LOSS train 0.25874778665602205 valid 1.4896226432174444\n",
      "EPOCH 389:\n",
      "  batch 100 loss: 0.24662916608154772\n",
      "  batch 200 loss: 0.247026469707489\n",
      "LOSS train 0.247026469707489 valid 1.470060403458774\n",
      "EPOCH 390:\n",
      "  batch 100 loss: 0.246512061804533\n",
      "  batch 200 loss: 0.23453029569238423\n",
      "LOSS train 0.23453029569238423 valid 1.4717133082449436\n",
      "EPOCH 391:\n",
      "  batch 100 loss: 0.24657811284065245\n",
      "  batch 200 loss: 0.2431109596043825\n",
      "LOSS train 0.2431109596043825 valid 1.468468016013503\n",
      "EPOCH 392:\n",
      "  batch 100 loss: 0.26067471571266654\n",
      "  batch 200 loss: 0.23793478030711412\n",
      "LOSS train 0.23793478030711412 valid 1.4677303954958916\n",
      "EPOCH 393:\n",
      "  batch 100 loss: 0.23412140384316443\n",
      "  batch 200 loss: 0.2571573877334595\n",
      "LOSS train 0.2571573877334595 valid 1.466545356437564\n",
      "EPOCH 394:\n",
      "  batch 100 loss: 0.24081187315285205\n",
      "  batch 200 loss: 0.26012551933526995\n",
      "LOSS train 0.26012551933526995 valid 1.4640307240188122\n",
      "EPOCH 395:\n",
      "  batch 100 loss: 0.22655129924416542\n",
      "  batch 200 loss: 0.2382426780462265\n",
      "LOSS train 0.2382426780462265 valid 1.4752863086760044\n",
      "EPOCH 396:\n",
      "  batch 100 loss: 0.23623969294130803\n",
      "  batch 200 loss: 0.2442354517430067\n",
      "LOSS train 0.2442354517430067 valid 1.4711529538035393\n",
      "EPOCH 397:\n",
      "  batch 100 loss: 0.24111889712512494\n",
      "  batch 200 loss: 0.2669987874478102\n",
      "LOSS train 0.2669987874478102 valid 1.4853405319154263\n",
      "EPOCH 398:\n",
      "  batch 100 loss: 0.24051375158131122\n",
      "  batch 200 loss: 0.2542874833196402\n",
      "LOSS train 0.2542874833196402 valid 1.4746544575318694\n",
      "EPOCH 399:\n",
      "  batch 100 loss: 0.23153864860534668\n",
      "  batch 200 loss: 0.23926217354834078\n",
      "LOSS train 0.23926217354834078 valid 1.4648860152810812\n",
      "EPOCH 400:\n",
      "  batch 100 loss: 0.2577750913798809\n",
      "  batch 200 loss: 0.24064603492617606\n",
      "LOSS train 0.24064603492617606 valid 1.4712323006242514\n",
      "EPOCH 401:\n",
      "  batch 100 loss: 0.23117964666336774\n",
      "  batch 200 loss: 0.25901848785579207\n",
      "LOSS train 0.25901848785579207 valid 1.4636705312877893\n",
      "EPOCH 402:\n",
      "  batch 100 loss: 0.24078512236475944\n",
      "  batch 200 loss: 0.25486908972263334\n",
      "LOSS train 0.25486908972263334 valid 1.4758453778922558\n",
      "EPOCH 403:\n",
      "  batch 100 loss: 0.23820834778249264\n",
      "  batch 200 loss: 0.24136896058917046\n",
      "LOSS train 0.24136896058917046 valid 1.4707025336101651\n",
      "EPOCH 404:\n",
      "  batch 100 loss: 0.24955411955714227\n",
      "  batch 200 loss: 0.22099623516201972\n",
      "LOSS train 0.22099623516201972 valid 1.4665047973394394\n",
      "EPOCH 405:\n",
      "  batch 100 loss: 0.25482495047152043\n",
      "  batch 200 loss: 0.23851874530315398\n",
      "LOSS train 0.23851874530315398 valid 1.4631795138120651\n",
      "EPOCH 406:\n",
      "  batch 100 loss: 0.24083676807582377\n",
      "  batch 200 loss: 0.24903679050505162\n",
      "LOSS train 0.24903679050505162 valid 1.4769245591014624\n",
      "EPOCH 407:\n",
      "  batch 100 loss: 0.23033323526382446\n",
      "  batch 200 loss: 0.25140175692737105\n",
      "LOSS train 0.25140175692737105 valid 1.4717917125672102\n",
      "EPOCH 408:\n",
      "  batch 100 loss: 0.25099352285265925\n",
      "  batch 200 loss: 0.24206846348941327\n",
      "LOSS train 0.24206846348941327 valid 1.4783225040882826\n",
      "EPOCH 409:\n",
      "  batch 100 loss: 0.25946391649544237\n",
      "  batch 200 loss: 0.22173493221402168\n",
      "LOSS train 0.22173493221402168 valid 1.4783487301319838\n",
      "EPOCH 410:\n",
      "  batch 100 loss: 0.25419710606336593\n",
      "  batch 200 loss: 0.23824063919484614\n",
      "LOSS train 0.23824063919484614 valid 1.4677088093012571\n",
      "EPOCH 411:\n",
      "  batch 100 loss: 0.267474493086338\n",
      "  batch 200 loss: 0.22855599887669087\n",
      "LOSS train 0.22855599887669087 valid 1.481295838020742\n",
      "EPOCH 412:\n",
      "  batch 100 loss: 0.2339538237452507\n",
      "  batch 200 loss: 0.24651871401816605\n",
      "LOSS train 0.24651871401816605 valid 1.4685549773275852\n",
      "EPOCH 413:\n",
      "  batch 100 loss: 0.24540133342146875\n",
      "  batch 200 loss: 0.23946196340024473\n",
      "LOSS train 0.23946196340024473 valid 1.4903991986066103\n",
      "EPOCH 414:\n",
      "  batch 100 loss: 0.24078929871320726\n",
      "  batch 200 loss: 0.24397703535854817\n",
      "LOSS train 0.24397703535854817 valid 1.4724878054112196\n",
      "EPOCH 415:\n",
      "  batch 100 loss: 0.2300455667823553\n",
      "  batch 200 loss: 0.25346228435635565\n",
      "LOSS train 0.25346228435635565 valid 1.4683826081454754\n",
      "EPOCH 416:\n",
      "  batch 100 loss: 0.25507597014307976\n",
      "  batch 200 loss: 0.23587743155658245\n",
      "LOSS train 0.23587743155658245 valid 1.4682577587664127\n",
      "EPOCH 417:\n",
      "  batch 100 loss: 0.2482832257449627\n",
      "  batch 200 loss: 0.24601498544216155\n",
      "LOSS train 0.24601498544216155 valid 1.474506737664342\n",
      "EPOCH 418:\n",
      "  batch 100 loss: 0.22607434801757337\n",
      "  batch 200 loss: 0.25765771958976985\n",
      "LOSS train 0.25765771958976985 valid 1.465944105759263\n",
      "EPOCH 419:\n",
      "  batch 100 loss: 0.2387960833311081\n",
      "  batch 200 loss: 0.24279766481369733\n",
      "LOSS train 0.24279766481369733 valid 1.4852741286158562\n",
      "EPOCH 420:\n",
      "  batch 100 loss: 0.24403789184987545\n",
      "  batch 200 loss: 0.25503592140972614\n",
      "LOSS train 0.25503592140972614 valid 1.4660372734069824\n",
      "EPOCH 421:\n",
      "  batch 100 loss: 0.25564924225211144\n",
      "  batch 200 loss: 0.24178223751485348\n",
      "LOSS train 0.24178223751485348 valid 1.4731745338067412\n",
      "EPOCH 422:\n",
      "  batch 100 loss: 0.2592726977914572\n",
      "  batch 200 loss: 0.22695576176047325\n",
      "LOSS train 0.22695576176047325 valid 1.4731183722615242\n",
      "EPOCH 423:\n",
      "  batch 100 loss: 0.24080776765942574\n",
      "  batch 200 loss: 0.2605701570957899\n",
      "LOSS train 0.2605701570957899 valid 1.4742140900343657\n",
      "EPOCH 424:\n",
      "  batch 100 loss: 0.2436786349862814\n",
      "  batch 200 loss: 0.2275809860229492\n",
      "LOSS train 0.2275809860229492 valid 1.4642228949815035\n",
      "EPOCH 425:\n",
      "  batch 100 loss: 0.24825735531747342\n",
      "  batch 200 loss: 0.24148668557405473\n",
      "LOSS train 0.24148668557405473 valid 1.471015077084303\n",
      "EPOCH 426:\n",
      "  batch 100 loss: 0.2463311628252268\n",
      "  batch 200 loss: 0.24390542093664408\n",
      "LOSS train 0.24390542093664408 valid 1.4648609962314367\n",
      "EPOCH 427:\n",
      "  batch 100 loss: 0.244408895149827\n",
      "  batch 200 loss: 0.24761996403336525\n",
      "LOSS train 0.24761996403336525 valid 1.477079065516591\n",
      "EPOCH 428:\n",
      "  batch 100 loss: 0.23524292916059494\n",
      "  batch 200 loss: 0.25741477876901625\n",
      "LOSS train 0.25741477876901625 valid 1.4849587641656399\n",
      "EPOCH 429:\n",
      "  batch 100 loss: 0.24034285478293896\n",
      "  batch 200 loss: 0.24365310154855252\n",
      "LOSS train 0.24365310154855252 valid 1.4823430012911558\n",
      "EPOCH 430:\n",
      "  batch 100 loss: 0.22537323139607907\n",
      "  batch 200 loss: 0.2511711051315069\n",
      "LOSS train 0.2511711051315069 valid 1.4749334892258048\n",
      "EPOCH 431:\n",
      "  batch 100 loss: 0.23106511805206537\n",
      "  batch 200 loss: 0.2531099132448435\n",
      "LOSS train 0.2531099132448435 valid 1.4711970090866089\n",
      "EPOCH 432:\n",
      "  batch 100 loss: 0.2490463723242283\n",
      "  batch 200 loss: 0.24396455965936184\n",
      "LOSS train 0.24396455965936184 valid 1.4854768943041563\n",
      "EPOCH 433:\n",
      "  batch 100 loss: 0.25604140110313894\n",
      "  batch 200 loss: 0.24087872989475728\n",
      "LOSS train 0.24087872989475728 valid 1.4831998441368341\n",
      "EPOCH 434:\n",
      "  batch 100 loss: 0.23488952554762363\n",
      "  batch 200 loss: 0.25452145032584667\n",
      "LOSS train 0.25452145032584667 valid 1.4745333585888147\n",
      "EPOCH 435:\n",
      "  batch 100 loss: 0.23607384335249662\n",
      "  batch 200 loss: 0.2442475987970829\n",
      "LOSS train 0.2442475987970829 valid 1.470602571964264\n",
      "EPOCH 436:\n",
      "  batch 100 loss: 0.24867133967578411\n",
      "  batch 200 loss: 0.24355129823088645\n",
      "LOSS train 0.24355129823088645 valid 1.4859804417937994\n",
      "EPOCH 437:\n",
      "  batch 100 loss: 0.24671528674662113\n",
      "  batch 200 loss: 0.25444907367229463\n",
      "LOSS train 0.25444907367229463 valid 1.474903304129839\n",
      "EPOCH 438:\n",
      "  batch 100 loss: 0.24857771560549735\n",
      "  batch 200 loss: 0.2295409306138754\n",
      "LOSS train 0.2295409306138754 valid 1.4709537122398615\n",
      "EPOCH 439:\n",
      "  batch 100 loss: 0.2268248099088669\n",
      "  batch 200 loss: 0.24728233564645052\n",
      "LOSS train 0.24728233564645052 valid 1.4665953796356916\n",
      "EPOCH 440:\n",
      "  batch 100 loss: 0.24009500652551652\n",
      "  batch 200 loss: 0.23927790649235248\n",
      "LOSS train 0.23927790649235248 valid 1.4743888564407825\n",
      "EPOCH 441:\n",
      "  batch 100 loss: 0.24489785619080068\n",
      "  batch 200 loss: 0.2469380657374859\n",
      "LOSS train 0.2469380657374859 valid 1.4681351743638515\n",
      "EPOCH 442:\n",
      "  batch 100 loss: 0.23697807818651198\n",
      "  batch 200 loss: 0.2594279722124338\n",
      "LOSS train 0.2594279722124338 valid 1.4718098426237702\n",
      "EPOCH 443:\n",
      "  batch 100 loss: 0.2354575376957655\n",
      "  batch 200 loss: 0.2633667754381895\n",
      "LOSS train 0.2633667754381895 valid 1.4698629658669233\n",
      "EPOCH 444:\n",
      "  batch 100 loss: 0.25539932064712045\n",
      "  batch 200 loss: 0.23024810872972012\n",
      "LOSS train 0.23024810872972012 valid 1.4750084783881903\n",
      "EPOCH 445:\n",
      "  batch 100 loss: 0.227195707783103\n",
      "  batch 200 loss: 0.25300411581993104\n",
      "LOSS train 0.25300411581993104 valid 1.489674774929881\n",
      "EPOCH 446:\n",
      "  batch 100 loss: 0.23860937997698783\n",
      "  batch 200 loss: 0.2431528890132904\n",
      "LOSS train 0.2431528890132904 valid 1.4746136739850044\n",
      "EPOCH 447:\n",
      "  batch 100 loss: 0.23953953944146633\n",
      "  batch 200 loss: 0.25488825649023056\n",
      "LOSS train 0.25488825649023056 valid 1.4658950939774513\n",
      "EPOCH 448:\n",
      "  batch 100 loss: 0.23579553619027138\n",
      "  batch 200 loss: 0.25694747544825075\n",
      "LOSS train 0.25694747544825075 valid 1.4684503693133593\n",
      "EPOCH 449:\n",
      "  batch 100 loss: 0.24448196694254876\n",
      "  batch 200 loss: 0.24954078443348407\n",
      "LOSS train 0.24954078443348407 valid 1.4734635651111603\n",
      "EPOCH 450:\n",
      "  batch 100 loss: 0.2516137458384037\n",
      "  batch 200 loss: 0.23542729370296\n",
      "LOSS train 0.23542729370296 valid 1.4729333072900772\n",
      "EPOCH 451:\n",
      "  batch 100 loss: 0.25009793147444725\n",
      "  batch 200 loss: 0.24721504002809525\n",
      "LOSS train 0.24721504002809525 valid 1.469700176268816\n",
      "EPOCH 452:\n",
      "  batch 100 loss: 0.2548997906595469\n",
      "  batch 200 loss: 0.2389536042883992\n",
      "LOSS train 0.2389536042883992 valid 1.4685568809509277\n",
      "EPOCH 453:\n",
      "  batch 100 loss: 0.23295207537710666\n",
      "  batch 200 loss: 0.2506137635558844\n",
      "LOSS train 0.2506137635558844 valid 1.470900995656848\n",
      "EPOCH 454:\n",
      "  batch 100 loss: 0.24170327618718146\n",
      "  batch 200 loss: 0.22661810427904128\n",
      "LOSS train 0.22661810427904128 valid 1.4692717120051384\n",
      "EPOCH 455:\n",
      "  batch 100 loss: 0.2341367620974779\n",
      "  batch 200 loss: 0.2522726698219776\n",
      "LOSS train 0.2522726698219776 valid 1.4661999698728323\n",
      "EPOCH 456:\n",
      "  batch 100 loss: 0.2514135644584894\n",
      "  batch 200 loss: 0.23471080467104913\n",
      "LOSS train 0.23471080467104913 valid 1.4652835838496685\n",
      "EPOCH 457:\n",
      "  batch 100 loss: 0.25908899925649165\n",
      "  batch 200 loss: 0.23744389794766904\n",
      "LOSS train 0.23744389794766904 valid 1.4719334095716476\n",
      "EPOCH 458:\n",
      "  batch 100 loss: 0.2332713669911027\n",
      "  batch 200 loss: 0.24789097659289838\n",
      "LOSS train 0.24789097659289838 valid 1.4664191771298647\n",
      "EPOCH 459:\n",
      "  batch 100 loss: 0.24446836203336716\n",
      "  batch 200 loss: 0.23636071711778642\n",
      "LOSS train 0.23636071711778642 valid 1.4715699087828398\n",
      "EPOCH 460:\n",
      "  batch 100 loss: 0.2473191323131323\n",
      "  batch 200 loss: 0.23915623642504216\n",
      "LOSS train 0.23915623642504216 valid 1.472142819315195\n",
      "EPOCH 461:\n",
      "  batch 100 loss: 0.2439548070728779\n",
      "  batch 200 loss: 0.24901133835315703\n",
      "LOSS train 0.24901133835315703 valid 1.4673911929130554\n",
      "EPOCH 462:\n",
      "  batch 100 loss: 0.24082432396709919\n",
      "  batch 200 loss: 0.23543689668178558\n",
      "LOSS train 0.23543689668178558 valid 1.4648080579936504\n",
      "EPOCH 463:\n",
      "  batch 100 loss: 0.2591072450578213\n",
      "  batch 200 loss: 0.2331130377203226\n",
      "LOSS train 0.2331130377203226 valid 1.4832837535068393\n",
      "EPOCH 464:\n",
      "  batch 100 loss: 0.25561088424175976\n",
      "  batch 200 loss: 0.22866602905094624\n",
      "LOSS train 0.22866602905094624 valid 1.464556334540248\n",
      "EPOCH 465:\n",
      "  batch 100 loss: 0.24498889334499835\n",
      "  batch 200 loss: 0.24445049900561572\n",
      "LOSS train 0.24445049900561572 valid 1.4668838735669851\n",
      "EPOCH 466:\n",
      "  batch 100 loss: 0.2502064710110426\n",
      "  batch 200 loss: 0.24518651686608792\n",
      "LOSS train 0.24518651686608792 valid 1.4697465170174837\n",
      "EPOCH 467:\n",
      "  batch 100 loss: 0.25091165825724604\n",
      "  batch 200 loss: 0.24517795622348784\n",
      "LOSS train 0.24517795622348784 valid 1.4687073193490505\n",
      "EPOCH 468:\n",
      "  batch 100 loss: 0.2307750593870878\n",
      "  batch 200 loss: 0.2391159638017416\n",
      "LOSS train 0.2391159638017416 valid 1.467377895489335\n",
      "EPOCH 469:\n",
      "  batch 100 loss: 0.23282869838178158\n",
      "  batch 200 loss: 0.2381095663458109\n",
      "LOSS train 0.2381095663458109 valid 1.4791718665510416\n",
      "EPOCH 470:\n",
      "  batch 100 loss: 0.2553950338810682\n",
      "  batch 200 loss: 0.23929057322442532\n",
      "LOSS train 0.23929057322442532 valid 1.4680925495922565\n",
      "EPOCH 471:\n",
      "  batch 100 loss: 0.23568809121847153\n",
      "  batch 200 loss: 0.2405080007016659\n",
      "LOSS train 0.2405080007016659 valid 1.4677034206688404\n",
      "EPOCH 472:\n",
      "  batch 100 loss: 0.2500956221669912\n",
      "  batch 200 loss: 0.23784318156540393\n",
      "LOSS train 0.23784318156540393 valid 1.4719214010983706\n",
      "EPOCH 473:\n",
      "  batch 100 loss: 0.24646567039191722\n",
      "  batch 200 loss: 0.2391959534585476\n",
      "LOSS train 0.2391959534585476 valid 1.4654859993606806\n",
      "EPOCH 474:\n",
      "  batch 100 loss: 0.25810285679996015\n",
      "  batch 200 loss: 0.23976898565888405\n",
      "LOSS train 0.23976898565888405 valid 1.4747380465269089\n",
      "EPOCH 475:\n",
      "  batch 100 loss: 0.22843787863850593\n",
      "  batch 200 loss: 0.257972616776824\n",
      "LOSS train 0.257972616776824 valid 1.475082972086966\n",
      "EPOCH 476:\n",
      "  batch 100 loss: 0.25144473418593405\n",
      "  batch 200 loss: 0.25815015852451323\n",
      "LOSS train 0.25815015852451323 valid 1.4752913787961006\n",
      "EPOCH 477:\n",
      "  batch 100 loss: 0.23734725587069988\n",
      "  batch 200 loss: 0.2437819951027632\n",
      "LOSS train 0.2437819951027632 valid 1.4761555306613445\n",
      "EPOCH 478:\n",
      "  batch 100 loss: 0.259570841640234\n",
      "  batch 200 loss: 0.24477449093014003\n",
      "LOSS train 0.24477449093014003 valid 1.4679794646799564\n",
      "EPOCH 479:\n",
      "  batch 100 loss: 0.24404273621737957\n",
      "  batch 200 loss: 0.2410203367471695\n",
      "LOSS train 0.2410203367471695 valid 1.468895722180605\n",
      "EPOCH 480:\n",
      "  batch 100 loss: 0.24166343323886394\n",
      "  batch 200 loss: 0.2669115234166384\n",
      "LOSS train 0.2669115234166384 valid 1.4669064600020647\n",
      "EPOCH 481:\n",
      "  batch 100 loss: 0.2536716890335083\n",
      "  batch 200 loss: 0.2450966064631939\n",
      "LOSS train 0.2450966064631939 valid 1.4694675672799349\n",
      "EPOCH 482:\n",
      "  batch 100 loss: 0.2400216843932867\n",
      "  batch 200 loss: 0.2639898113906384\n",
      "LOSS train 0.2639898113906384 valid 1.4757591597735882\n",
      "EPOCH 483:\n",
      "  batch 100 loss: 0.25225521944463253\n",
      "  batch 200 loss: 0.23830894853919746\n",
      "LOSS train 0.23830894853919746 valid 1.4671601466834545\n",
      "EPOCH 484:\n",
      "  batch 100 loss: 0.24196807742118837\n",
      "  batch 200 loss: 0.24697247594594957\n",
      "LOSS train 0.24697247594594957 valid 1.4684788100421429\n",
      "EPOCH 485:\n",
      "  batch 100 loss: 0.2253179804980755\n",
      "  batch 200 loss: 0.25991673111915586\n",
      "LOSS train 0.25991673111915586 valid 1.4756761491298676\n",
      "EPOCH 486:\n",
      "  batch 100 loss: 0.23798914968967438\n",
      "  batch 200 loss: 0.240707470998168\n",
      "LOSS train 0.240707470998168 valid 1.4684346113353968\n",
      "EPOCH 487:\n",
      "  batch 100 loss: 0.256592741087079\n",
      "  batch 200 loss: 0.22958738937973977\n",
      "LOSS train 0.22958738937973977 valid 1.485550507903099\n",
      "EPOCH 488:\n",
      "  batch 100 loss: 0.25637728720903397\n",
      "  batch 200 loss: 0.23046392373740673\n",
      "LOSS train 0.23046392373740673 valid 1.4656457025557756\n",
      "EPOCH 489:\n",
      "  batch 100 loss: 0.2623519814759493\n",
      "  batch 200 loss: 0.23006513841450216\n",
      "LOSS train 0.23006513841450216 valid 1.4730474539101124\n",
      "EPOCH 490:\n",
      "  batch 100 loss: 0.24760894268751144\n",
      "  batch 200 loss: 0.2574600634723902\n",
      "LOSS train 0.2574600634723902 valid 1.470161547884345\n",
      "EPOCH 491:\n",
      "  batch 100 loss: 0.24301478497684\n",
      "  batch 200 loss: 0.22667727634310722\n",
      "LOSS train 0.22667727634310722 valid 1.4812012668699026\n",
      "EPOCH 492:\n",
      "  batch 100 loss: 0.2453321471065283\n",
      "  batch 200 loss: 0.23526551850140096\n",
      "LOSS train 0.23526551850140096 valid 1.4764933437108994\n",
      "EPOCH 493:\n",
      "  batch 100 loss: 0.2534351149946451\n",
      "  batch 200 loss: 0.23687253952026366\n",
      "LOSS train 0.23687253952026366 valid 1.4632161473855376\n",
      "EPOCH 494:\n",
      "  batch 100 loss: 0.2511400265991688\n",
      "  batch 200 loss: 0.24188818097114562\n",
      "LOSS train 0.24188818097114562 valid 1.4757595416158438\n",
      "EPOCH 495:\n",
      "  batch 100 loss: 0.23210381001234054\n",
      "  batch 200 loss: 0.2490809842199087\n",
      "LOSS train 0.2490809842199087 valid 1.4707277696579695\n",
      "EPOCH 496:\n",
      "  batch 100 loss: 0.2504657193273306\n",
      "  batch 200 loss: 0.23589151598513125\n",
      "LOSS train 0.23589151598513125 valid 1.4741425681859255\n",
      "EPOCH 497:\n",
      "  batch 100 loss: 0.2635830494761467\n",
      "  batch 200 loss: 0.23469245709478856\n",
      "LOSS train 0.23469245709478856 valid 1.472040350548923\n",
      "EPOCH 498:\n",
      "  batch 100 loss: 0.24871729999780656\n",
      "  batch 200 loss: 0.234835162460804\n",
      "LOSS train 0.234835162460804 valid 1.4681416489183903\n",
      "EPOCH 499:\n",
      "  batch 100 loss: 0.23844076573848724\n",
      "  batch 200 loss: 0.2381850282102823\n",
      "LOSS train 0.2381850282102823 valid 1.4658693838864565\n",
      "EPOCH 500:\n",
      "  batch 100 loss: 0.24405334115028382\n",
      "  batch 200 loss: 0.2592009626328945\n",
      "LOSS train 0.2592009626328945 valid 1.4744241535663605\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "epoch_number = 0\n",
    "EPOCHS = 500\n",
    "\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "    train_loss_values.append(avg_loss)\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(\"cuda:1\")\n",
    "            vlabels = vlabels.to(\"cuda:1\")\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs.squeeze(1), vlabels).item()\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    val_loss_values.append(avg_vloss)\n",
    "    \n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    # scheduler.step()\n",
    "    scheduler.step(avg_vloss)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the training and validation loss values\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot([val for val in val_loss_values], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Time for Quantum yield')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.578\n",
      "RMSE: 1.185\n",
      "Average test loss: 1.3806560933589935\n"
     ]
    }
   ],
   "source": [
    "# After training, evaluate the model on test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_test_loss = 0.0\n",
    "\n",
    "# Lists to store actual and predicted values\n",
    "actual_values = []\n",
    "predicted_values = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for i, data in enumerate(test_eval_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(\"cuda:1\")\n",
    "        labels = labels.to(\"cuda:1\")\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(1), labels)\n",
    "                \n",
    "        # Append actual and predicted values to lists\n",
    "        actual_values.extend(labels.cpu().numpy())\n",
    "        predicted_values.extend(outputs.squeeze(1).cpu().numpy())\n",
    "\n",
    "        running_test_loss += loss.item()\n",
    "\n",
    "# Calculate the average loss over the test dataset\n",
    "avg_test_loss = running_test_loss / (i + 1)\n",
    "\n",
    "# actual_values_unscaled = [i * std_train.item() + mean_train.item() for i in actual_values]\n",
    "# predicted_values_unscaled = [i * std_train.item() + mean_train.item() for i in predicted_values]\n",
    "\n",
    "# Unscale actual and predicted values after logarithmic transformation\n",
    "actual_values_unscaled = [np.exp(i) for i in actual_values]\n",
    "predicted_values_unscaled = [np.exp(i) for i in predicted_values]\n",
    "\n",
    "# Compute R^2 and RMSE\n",
    "r2 = np.corrcoef(actual_values, predicted_values)[0,1] **2\n",
    "rmse = root_mean_squared_error(actual_values, predicted_values)\n",
    "\n",
    "print(f\"R^2: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"Average test loss: {avg_test_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
